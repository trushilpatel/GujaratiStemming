{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data Cleaning, Gujarati Stemming, Word2Vec and Xgram",
      "provenance": [],
      "collapsed_sections": [
        "uWxU9Ch6E9iQ",
        "YZXuVqX2DOYb",
        "wqbeHqSPCXhN",
        "iN5Cmdsd7I95",
        "H3JIG6S0EQxk",
        "8sVB2hLVD_w8",
        "Ibd32I1m7diO",
        "d2C-Nr-GMdGv",
        "MlAbHTMXHQEi",
        "kWhZj65uM6rB",
        "Kuf8mTx5CG-8",
        "xOMFh3cLDFx7",
        "4_7DRDGGDNJ6",
        "VtcxcXX9DRQ1",
        "e9yq0hnHDYZB",
        "B44Uvxd4DxCR",
        "_6ZeEy8ZD866",
        "UkFUBtGzD_4H",
        "1-CQpwH3EKPb",
        "2MRrZ7VLEpG-",
        "3vX2EcFIEN-C",
        "osoVryqbEhPe",
        "lBjgauEcE1Lp",
        "t1HDix4GDmwo"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWxU9Ch6E9iQ",
        "colab_type": "text"
      },
      "source": [
        "# Google file download"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "956XNw56FEe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "def downloadFile(fileName):\n",
        "  files.download(fileName)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZXuVqX2DOYb",
        "colab_type": "text"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PT1jeQUDPOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tokenizer:\n",
        "    # DONE\n",
        "    def sentenceTokenizer(self, text='', file=\"\", save_as_file=''):\n",
        "        \"\"\"\n",
        "\n",
        "        :param text: [OPTIONAL] for text's sentence Tokenizing\n",
        "        :param file: [OPTIONAL] for given files sentence tokenizing\n",
        "        :param save_as_file: [OPTIONAL] location to save tokenized sentences\n",
        "        :return: [OPTIONAL] tokenized sentences either in list or saved in given file location\n",
        "\n",
        "        Notice : Enter the file name with it's extension\n",
        "        \"\"\"\n",
        "\n",
        "        # Preparing a sentence_data\n",
        "        if file != '':\n",
        "            open_file = open(file, 'rt', encoding=\"utf8\")\n",
        "            sentences_data = open_file.read()\n",
        "        else:\n",
        "            sentences_data = text\n",
        "\n",
        "        # Creating TokenizerHelper object\n",
        "        tz = SentenceTokenizerHelper()\n",
        "\n",
        "        if save_as_file == '':\n",
        "            return tz.sentenceTokenizerHelper(sentences_data)\n",
        "        else:\n",
        "            file = open(save_as_file, 'w', encoding=\"utf8\")\n",
        "            for tokenized_sentences in tz.sentenceTokenizerHelper(sentences_data):\n",
        "                file.write(tokenized_sentences)\n",
        "                file.write(\"\\n\")\n",
        "            file.close()\n",
        "\n",
        "    # DONE\n",
        "    def wordTokenizer(self, sentences='', file=''):\n",
        "\n",
        "        \"\"\"\n",
        "         Here only one argument is possible sentences or file if both were given then\n",
        "         the sentences will be executed\n",
        "\n",
        "        :param file: [OPTIONAL] get the sentenceTokenizer's output file\n",
        "        :param sentences: [OPTIONAL] takes output list of sentenceTokenizer's output\n",
        "        :return: word tokenized list is returned\n",
        "\n",
        "        ;future : special character remover\n",
        "\n",
        "        \"\"\"\n",
        "        if sentences != '':\n",
        "            pass\n",
        "        elif file != '':\n",
        "            sentences = []\n",
        "            file = open(file, 'rt', encoding='utf-8')\n",
        "            for sentence in file.readlines():\n",
        "                sentences.append(sentence)\n",
        "\n",
        "        wth = WordTokenizerHelper()\n",
        "        tokenized_words = []\n",
        "        for sentence in sentences:\n",
        "            helper_output = wth.wordTokenizerHelper(sentence)\n",
        "            if helper_output != []:\n",
        "                tokenized_words.append(helper_output)\n",
        "\n",
        "        return tokenized_words\n",
        "\n",
        "\n",
        "class SentenceTokenizerHelper:  # DONE\n",
        "    def sentenceTokenizerHelper(self, sentence_data):\n",
        "        \"\"\"\n",
        "\n",
        "        :param sentence_data: text of sentences as a input\n",
        "        :return: list of tokenized sentences\n",
        "        \"\"\"\n",
        "        # replacing \\n in sentences and spliting it with \".\"\n",
        "        sentence_data = sentence_data.replace(\"\\n\", \".\")\n",
        "        tokenized_sentence_list = sentence_data.split(\".\")\n",
        "\n",
        "        # removing extra space from starting and ending\n",
        "        for i in range(len(tokenized_sentence_list)):\n",
        "            result = tokenized_sentence_list[i].strip()\n",
        "            # removing line which is empty line\n",
        "            if result != '':\n",
        "                tokenized_sentence_list[i] = tokenized_sentence_list[i].strip()\n",
        "\n",
        "        # Here assume last element as a \"zyx.\\n\"\n",
        "        # after removing \\n\n",
        "        # this will be taken as [\"zyx.\",\"\"]\n",
        "        # so we must have to remove this \"\" element which is empty string\n",
        "        if tokenized_sentence_list[-1] == \"\":\n",
        "            tokenized_sentence_list.pop(-1)\n",
        "\n",
        "        return tokenized_sentence_list\n",
        "\n",
        "\n",
        "class WordTokenizerHelper:  # DONE\n",
        "    \"\"\"\n",
        "    end_special_characters :\n",
        "            this are the characters which occurs at the end of any words\n",
        "    start_special_character :\n",
        "            this are the characters which occurs at the starting of any words\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        # add features to remove more than one end or start special_characters\n",
        "        self.end_special_characters = (\",\", \".\", \"?\", \")\", \"!\", '\"', \"'\", \"]\", \"}\", \";\", \":\", \"•\")\n",
        "        self.start_special_characters = (\"'\", '\"', \"(\", \"[\", \"{\", \"•\", \"!\", \"#\", \"|\", \"-\", '~', '}')\n",
        "\n",
        "    def wordTokenizerHelper(self, sentence):\n",
        "        # here sentence.split() splits sentence and creates sentence's word list\n",
        "        tokenized_words = sentence.split()\n",
        "\n",
        "        # removing starting special characters\n",
        "        for word in tokenized_words:\n",
        "            for ssc in self.start_special_characters:\n",
        "                if word.startswith(ssc):\n",
        "                    tokenized_words[tokenized_words.index(word)] = word.replace(ssc, \"\")\n",
        "                    break\n",
        "\n",
        "        # removing ending special characters\n",
        "        for word in tokenized_words:\n",
        "            for esc in self.end_special_characters:\n",
        "                if word.endswith(esc):\n",
        "                    tokenized_words[tokenized_words.index(word)] = word.replace(esc, \"\")\n",
        "                    break\n",
        "        return tokenized_words\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqbeHqSPCXhN",
        "colab_type": "text"
      },
      "source": [
        "# Stop Words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8oGdUXjCa3q",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "source": [
        "class StopWords:\n",
        "    def __init__(self):\n",
        "        self.stopWordsLetterDict = readJsonFile(\n",
        "            file_name=r\"/content/drive/My Drive/Gujarati chatbot/stemmer2/stopWord/stopWords_letters_python_dictionary.json\"\n",
        "        )\n",
        "        self.suffix = Suffix()\n",
        "\n",
        "    def isValidStopWord(self, word):\n",
        "        \"\"\"\n",
        "\n",
        "        :param word: gets word as input\n",
        "        :return: True/False based on is valid stop word or not\n",
        "        \"\"\"\n",
        "        word = word.strip()\n",
        "        wordLen = len(word)\n",
        "\n",
        "        if wordLen == 0:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            temp_dict = self.stopWordsLetterDict.get(word[0])\n",
        "\n",
        "            if temp_dict is None:\n",
        "                return False\n",
        "\n",
        "            lastEndWordIndex = 0\n",
        "            for letter_index in range(1, wordLen):\n",
        "\n",
        "                get_letter_dict = temp_dict.get(word[letter_index])\n",
        "\n",
        "                if get_letter_dict is None:\n",
        "                    if self.suffix.isValidSuffix(word[lastEndWordIndex + 1:]):\n",
        "                        return True\n",
        "                    return False\n",
        "\n",
        "                temp_dict = get_letter_dict\n",
        "\n",
        "                if temp_dict.get('end'):\n",
        "                    lastEndWordIndex = letter_index\n",
        "\n",
        "            else:\n",
        "                if lastEndWordIndex + 1 == wordLen:\n",
        "                    return True\n",
        "                elif self.suffix.isValidSuffix(word[lastEndWordIndex + 1:]):\n",
        "                    return True\n",
        "\n",
        "            return False\n",
        "        except:\n",
        "            print(\"ERROR: IN STOPWORDS\")\n",
        "            return False\n",
        "\n",
        "    def removeStopWordsFromList(self, words: list):\n",
        "        \"\"\"\n",
        "\n",
        "        :param words: list of tokenized word\n",
        "                      EX: [w1,w2]\n",
        "        :return: None\n",
        "                cause it directly made changes to the list\n",
        "\n",
        "                SO BE CAREFUL WHILE USING THIS FUNCTION\n",
        "        \"\"\"\n",
        "        for wordIndex in range(len(words) - 1, -1, -1):\n",
        "            if self.isValidStopWord(words[wordIndex]):\n",
        "                words.pop(wordIndex)\n",
        "\n",
        "    def removeStopWordsFromTokenizedWords(self, tokenizedWords: list):\n",
        "        \"\"\"\n",
        "\n",
        "        :param tokenizedWords: list of tokenized sentences into words\n",
        "                               EX: [[w1,w2,..],[w3,w4,..],..]\n",
        "        :return: None\n",
        "               cause it directly made changes to the list\n",
        "\n",
        "                SO BE CAREFUL WHILE USING THIS FUNCTION\n",
        "        \"\"\"\n",
        "        for tw in tokenizedWords:\n",
        "            self.removeStopWordsFromList(tw)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN5Cmdsd7I95",
        "colab_type": "text"
      },
      "source": [
        "# possible Gujarati Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GrduH7F7GNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getPossGujChars(filePath=None):\n",
        "    allPossGujChars = \"ૌ  ૈ ા ી ૂ બ હ ગ દ જ ડ ઼ ૉ ો ે ્ િ ુ પ ર ક ત ચ ટ ં \" \\\n",
        "                      \"મ ન વ લ સ ય ઍ ૅ ્ ર જ ત ક શ ઃ ઋ ઔ ઐ આ ઈ ઊ \" \\\n",
        "                      \"ભ ઙ ઘ ધ ઝ ઢ ઞ ઑ ઓ એ અ ઇ ઉ ફ ખ થ છ ઠ ઁ ણ ળ શ ષ ઠ\"\n",
        "\n",
        "    characters = list(set(map(str.strip, allPossGujChars.split())))\n",
        "    characters.sort()\n",
        "\n",
        "    v = ''\n",
        "    characters = {k: v for k in allPossGujChars}\n",
        "    # print(\"Total Characters :\", len(characters))\n",
        "    # print(\"All characters : \\n\", characters)\n",
        "\n",
        "    if filePath is not None:\n",
        "        file = open(filePath, 'wt', encoding='utf-8')\n",
        "\n",
        "        for i in characters:\n",
        "            file.write(i + '\\n')\n",
        "        file.close()\n",
        "\n",
        "    else:\n",
        "        return characters\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrZ9PdUd7CUO",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5Fzv7Ai7Ev5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataCleaner:\n",
        "    def __init__(self):\n",
        "        self.possGujChars = getPossGujChars()\n",
        "        self.punctuations = '''!()-[]\\\\{};:'\",<>./?@#$%^&*_~‘ \\n'''\n",
        "        # here in punctuation set ':' as per the dataset\n",
        "\n",
        "    def isGujaratiWord(self, word):\n",
        "        for char in word:\n",
        "            if self.possGujChars.get(char) is None:\n",
        "                break\n",
        "        else:\n",
        "            return word\n",
        "        return ''\n",
        "\n",
        "    def cleanData(self, filePath=None, data=None,removeRedundancy=False, replaceNewLineUsing=None, splitSentencesUsing=None, joinSentencesUsing=None):\n",
        "        \"\"\"\n",
        "        get either a filePath or data to process\n",
        "        if both were given then FILE is preferred\n",
        "\n",
        "        :param filePath: [OPTIONAL] path of file to be cleaned\n",
        "        :param data: [OPTIONAL] data to clean\n",
        "        :return: if filePath : returns Nothing and writes a data to file directly\n",
        "                 if data : return cleaned data\n",
        "        \"\"\"\n",
        "\n",
        "        if filePath is None and data is None:\n",
        "            return\n",
        "        elif filePath is not None:\n",
        "            file = open(filePath, 'rt', encoding='utf-8')\n",
        "            text = file.read()\n",
        "            file.close()\n",
        "        else:\n",
        "            text = data\n",
        "        print(\"Total text length :\", len(text))\n",
        "\n",
        "        # make change here\n",
        "        if replaceNewLineUsing is not None:\n",
        "            text = text.replace('\\n', replaceNewLineUsing)\n",
        "\n",
        "        text = text.replace('(', ' ( ')\n",
        "        text = text.replace(')', ' ) ')\n",
        "        text = text.replace('[', ' [ ')\n",
        "        text = text.replace(']', ' ] ')\n",
        "        text = text.replace('{', ' { ')\n",
        "        text = text.replace('}', ' } ')\n",
        "\n",
        "        # MAKE CHANGE HERE\n",
        "        if splitSentencesUsing is not None:\n",
        "            sentences = text.split(splitSentencesUsing)\n",
        "        else:\n",
        "            sentences = text.split('\\n')\n",
        "\n",
        "        del text\n",
        "        print(\"Total sentences :\", len(sentences))\n",
        "\n",
        "        for sentenceIndex in range(len(sentences)):\n",
        "            if sentenceIndex % 1000 == 0:\n",
        "                print(sentenceIndex)\n",
        "            newSentence = ''\n",
        "            for word in sentences[sentenceIndex].split():\n",
        "                gujWord = self.isGujaratiWord(word.strip(self.punctuations))\n",
        "                if gujWord != '':\n",
        "                    newSentence += gujWord + ' '\n",
        "            if len(newSentence.split()) > 1:\n",
        "                sentences[sentenceIndex] = newSentence.strip()\n",
        "\n",
        "        if removeRedundancy is True:\n",
        "            sentences = list(set(sentences))\n",
        "\n",
        "        if joinSentencesUsing is None:\n",
        "            joinSentencesUsing = '.\\n'\n",
        "            \n",
        "        if filePath is not None:\n",
        "            file = open(filePath, 'wt', encoding='utf-8')\n",
        "            for sentence in sentences:\n",
        "                # MAKE CHANGES HERE\n",
        "                file.write(sentence + joinSentencesUsing)\n",
        "            file.close()\n",
        "        else:\n",
        "            text = ''\n",
        "            for sentence in sentences:\n",
        "                if sentence.strip() != '':\n",
        "                    # MAKE CHANGE HERE\n",
        "                    text += sentence + joinSentencesUsing\n",
        "            return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3JIG6S0EQxk",
        "colab_type": "text"
      },
      "source": [
        "# Read Write JSON file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEL6VqdoEXv6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "def writeJsonFile(data, file_name):\n",
        "    json_file = open(file_name, 'w')\n",
        "    json.dump(data, json_file)\n",
        "    json_file.close()\n",
        "\n",
        "\n",
        "def readJsonFile(file_name):\n",
        "    json_file = open(file_name, 'r')\n",
        "    data = json.load(json_file)\n",
        "    return data\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sVB2hLVD_w8",
        "colab_type": "text"
      },
      "source": [
        "# Suffix Removal\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yqhyw3ssEIWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Suffix:\n",
        "    def __init__(self):\n",
        "        self.suffixLettersDict = readJsonFile(\n",
        "            file_name=r\"/content/drive/My Drive/Gujarati chatbot/stemmer2/suffix/suffix_letters_python_dictionary.json\"\n",
        "        )\n",
        "\n",
        "    def isValidSuffix(self, word):\n",
        "        word = word.strip()\n",
        "        wordLen = len(word)\n",
        "\n",
        "        if wordLen == 0:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            temp_dict = self.suffixLettersDict.get(word[0])\n",
        "\n",
        "            if temp_dict is None:\n",
        "                return False\n",
        "\n",
        "            lastEndWordIndex = 0\n",
        "\n",
        "            for letter_index in range(1, wordLen):\n",
        "\n",
        "                get_letter_dict = temp_dict.get(word[letter_index])\n",
        "\n",
        "                if get_letter_dict is None:\n",
        "                    return False\n",
        "                temp_dict = get_letter_dict\n",
        "\n",
        "                if temp_dict.get('end'):\n",
        "                    lastEndWordIndex = letter_index\n",
        "\n",
        "            else:\n",
        "                if lastEndWordIndex + 1 == wordLen:\n",
        "                    return True\n",
        "\n",
        "            return False\n",
        "        except:\n",
        "            print(\"ERROR: IN SUFFIX\")\n",
        "            return False\n",
        "\n",
        "    def findSuffixAndRemove(self, word):\n",
        "        for wordIndex in range(len(word)):\n",
        "            if self.isValidSuffix(word[wordIndex:]):\n",
        "                if wordIndex != 0:\n",
        "                    return word[0:wordIndex]\n",
        "        else:\n",
        "            return word\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ibd32I1m7diO",
        "colab_type": "text"
      },
      "source": [
        "# Gujarati Stemmer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW2-F3cB4xWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GujaratiStemmer:\n",
        "    def __init__(self):\n",
        "        self.guDict = readJsonFile(\n",
        "            r\"/content/drive/My Drive/Gujarati chatbot/stemmer2/helpingFiles/new_gu_dict_letters_python_dictionary.json\")\n",
        "        self.suffix = Suffix()\n",
        "\n",
        "    def wordStemmingException(self, word, lastLetterIndex):\n",
        "        temp = self.suffix.findSuffixAndRemove(word)\n",
        "        if temp == word:\n",
        "            temp = word[0:lastLetterIndex + 1]\n",
        "        return temp\n",
        "\n",
        "    def guWordStemmer(self, word):\n",
        "        word = word.strip()\n",
        "        wordLen = len(word)\n",
        "\n",
        "        if wordLen == 0:\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            temp_dict = self.guDict.get(word[0])\n",
        "\n",
        "            if temp_dict is None:\n",
        "                return False\n",
        "\n",
        "            lastLetterIndex = 0\n",
        "            for letter_index in range(1, wordLen):\n",
        "                get_letter_dict = temp_dict.get(word[letter_index])\n",
        "\n",
        "                if get_letter_dict is None:\n",
        "                    if self.suffix.isValidSuffix(word[lastLetterIndex + 1:]):\n",
        "                        return word[0:lastLetterIndex + 1]\n",
        "                    return self.wordStemmingException(word, lastLetterIndex)\n",
        "\n",
        "                temp_dict = get_letter_dict\n",
        "\n",
        "                if temp_dict.get('end'):\n",
        "                    lastLetterIndex = letter_index\n",
        "\n",
        "            else:\n",
        "                if lastLetterIndex + 1 == wordLen:\n",
        "                    return word\n",
        "                elif self.suffix.isValidSuffix(word[lastLetterIndex + 1:]):\n",
        "                    return word[0:lastLetterIndex + 1]\n",
        "\n",
        "            return self.wordStemmingException(word, lastLetterIndex)\n",
        "        except:\n",
        "            print(\"ERROR: IN GUJARATI STEMMER2\")\n",
        "            return self.wordStemmingException(word, lastLetterIndex)\n",
        "\n",
        "    def guWordsListStemmer(self, listOfWords):\n",
        "        \"\"\"\n",
        "\n",
        "        :param listOfWords: list of wordsd\n",
        "                            EX:- [w,a,...]\n",
        "        :return: it directly updates the list\n",
        "                 so returns None\n",
        "        \"\"\"\n",
        "        for wordIndex in range(len(listOfWords)):\n",
        "            listOfWords[wordIndex] = self.guWordStemmer(listOfWords[wordIndex])\n",
        "\n",
        "    def guTokenizedWordStemmer(self, tokenizedWords):\n",
        "        \"\"\"\n",
        "\n",
        "        :param tokenizedWords: list\n",
        "                               EX:- [[..],[..],..]\n",
        "        :return: it directly updates the list\n",
        "                 so returns None\n",
        "        \"\"\"\n",
        "        for wordsListIndex in range(len(tokenizedWords)):\n",
        "            self.guWordsListStemmer(tokenizedWords[wordsListIndex])\n",
        "\n",
        "    def guStemmer(self, filePath=None, text=None, cleanData=False, saveFileDir=None, saveFileName=None):\n",
        "        \"\"\"\n",
        "\n",
        "        :param filePath: [OPTIONAL] file's location\n",
        "        :param cleanData: [OPTIONAL] data needed to be cleaned or not\n",
        "        :param text: [OPTIONAL] text dataset\n",
        "        :return: stemmed text\n",
        "\n",
        "        FOR MEMORY SAVING FOLLOWING COULD BE HELPFUL [ YET TO ADD, IT's A NOTION]\n",
        "        --------------------------------------------\n",
        "        :param saveFileDir: save file directory\n",
        "        :param saveFileName: name of file\n",
        "        \"\"\"\n",
        "        if filePath is None and text is None:\n",
        "            raise Exception(\"ERROR in guStemmer\\n\\t Invalid Argument\")\n",
        "\n",
        "        # gathering data\n",
        "        if filePath is not None:\n",
        "            file = open(filePath, 'rt', encoding='utf-8')\n",
        "            text = file.read()\n",
        "            file.close()\n",
        "\n",
        "        # cleaning data\n",
        "        if cleanData is True:\n",
        "            dc = DataCleaner()\n",
        "            text = dc.cleanData(data=text)\n",
        "\n",
        "        # tokenizing the data\n",
        "        tokenizer = Tokenizer()\n",
        "        senTok = tokenizer.sentenceTokenizer(text=text)\n",
        "        worTok = tokenizer.wordTokenizer(sentences=senTok)\n",
        "        del senTok\n",
        "\n",
        "        # removing stopwords\n",
        "        sw = StopWords()\n",
        "        sw.removeStopWordsFromTokenizedWords(tokenizedWords=worTok)\n",
        "\n",
        "        # stemming whole tokenized words list\n",
        "        self.guTokenizedWordStemmer(tokenizedWords=worTok)\n",
        "\n",
        "        return worTok\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-Jn2-8H383d",
        "colab_type": "text"
      },
      "source": [
        "# Stem Word and it's all possible Unstem Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSDmXHRw4Gna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stemWordAndItsWords(filepath=None, data=None):\n",
        "    if filepath is None and data is None:\n",
        "        return None\n",
        "      \n",
        "    if filepath is not None:\n",
        "        file = open(filepath, 'rt', encoding='utf8')\n",
        "        data = file.read()\n",
        "    \n",
        "    cleanData = DataCleaner()\n",
        "    \n",
        "    data = cleanData.cleanData( data=data, removeRedundancy=True, joinSentencesUsing=' ')\n",
        "    data = data.split()\n",
        "\n",
        "    print(\"Total unique words :\", len(data))\n",
        "\n",
        "    gujStem = GujaratiStemmer()\n",
        "    stemWords = {}\n",
        "\n",
        "    for word in data:\n",
        "        stemWord = gujStem.guWordStemmer(word)\n",
        "\n",
        "        if stemWords.get(stemWord) is None:\n",
        "            stemWords[stemWord] = {}\n",
        "\n",
        "        if stemWords[stemWord].get(word) is None:\n",
        "            stemWords[stemWord].update({\n",
        "                word: 1\n",
        "            })\n",
        "        else:\n",
        "            stemWords[stemWord][word] += 1   \n",
        "    \n",
        "    stemWordsProb = {}\n",
        "    \n",
        "    for stemWord in stemWords:\n",
        "        totalPossibleWords = 0\n",
        "        for word in stemWords[stemWord]:\n",
        "          totalPossibleWords += stemWords[stemWord][word]\n",
        "\n",
        "        tempDict = {}\n",
        "        for word in stemWords[stemWord]:\n",
        "            tempDict[word] = stemWords[stemWord][word]/totalPossibleWords\n",
        "\n",
        "        stemWordsProb[stemWord] = tempDict.copy()\n",
        "    \n",
        "    return stemWords, stemWordsProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb9fULhJ74DN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemWords, stemWordsProb = stemWordAndItsWords(filepath=r\"/content/combined_cleanedDataSet.txt\")\n",
        "\n",
        "import pickle\n",
        "file = open(\"/content/drive/My Drive/Output/stemWordAngItsWords.pickle\",'wb')\n",
        "pickle.dump(stemWords, file)\n",
        "file.close()\n",
        "\n",
        "file = open(\"/content/drive/My Drive/Output/stemWordAngItsWordsProbability.pickle\",'wb')\n",
        "pickle.dump(stemWordsProb, file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KrMuGs8GA48",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "outputId": "db2299b8-ff29-421e-9161-406784165a3a"
      },
      "source": [
        "for w in list(stemWordsProb.keys())[0:30]:\n",
        "    print(w, stemWordsProb[w])\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "સૂત્ર {'સૂત્રો': 0.26828765711930763, 'સૂત્રોનું': 0.028229960848959408, 'સૂત્રોનો': 0.006181743251596951, 'સૂત્રોના': 0.26890583144446734, 'સૂત્રમાં': 0.017514939212858027, 'સૂત્રના': 0.011745312178034206, 'સૂત્ર': 0.16711312590150423, 'સૂત્રોની': 0.008242324335462601, 'સૂત્રને': 0.009272614877395426, 'સૂત્રનો': 0.007005975685143211, 'સૂત્રોનાં': 0.007624150010302906, 'સૂત્રોમાંથી': 0.017720997321244592, 'સૂત્રોએ': 0.13579229342674634, 'સૂત્રનું': 0.00576962703482382, 'સૂત્રોથી': 0.00329692973418504, 'સૂત્રોમાં': 0.006181743251596951, 'સૂત્રમાંનું': 0.00082423243354626, 'સૂત્રની': 0.00329692973418504, 'સૂત્રમાંથી': 0.00082423243354626, 'સૂત્રએ': 0.009478672985781991, 'સૂત્રોને': 0.00329692973418504, 'સૂત્રવાચન.': 0.000206058108386565, 'સૂત્રમય': 0.00123634865031939, 'સૂત્રાદિરજ્જીકરણ': 0.00041211621677313, 'સૂત્રસિદ્ધિ': 0.00041211621677313, 'સૂત્રનાં': 0.0010302905419328251, 'સૂત્રાદિ': 0.00041211621677313, 'સૂત્રા': 0.00123634865031939, 'સૂત્રવાળું': 0.000206058108386565, 'સૂત્રીકરણ': 0.000618174325159695, 'સૂત્રોમાંના': 0.00041211621677313, 'સૂત્રી': 0.000618174325159695, 'સૂત્રવિરુદ્ધ': 0.000206058108386565, 'સૂત્રે': 0.0020605810838656502, 'સૂત્ર.': 0.000206058108386565, 'સૂત્રવાળા': 0.000206058108386565, 'સૂત્રાહ': 0.00041211621677313, 'સૂત્રમધ્ય': 0.000206058108386565, 'સૂત્રથી': 0.001442406758705955, 'સૂત્રાભૂષણ': 0.00041211621677313, 'સૂત્રપ્રસિદ્ધ': 0.000206058108386565, 'સૂત્રસમૂહ': 0.000206058108386565, 'સૂત્રોઅમદાવાદ': 0.000206058108386565, 'સૂત્રવ્યાકરણ': 0.00041211621677313, 'સૂત્રગ્રહણ': 0.000206058108386565, 'સૂત્રયત્ન.': 0.000206058108386565}\n",
            "દ્વાર {'દ્વારા': 0.9739579398060698, 'દ્વાર': 0.019518952273013476, 'દ્વારે': 0.0014355874574990555, 'દ્વારની': 0.0008059438357889434, 'દ્વારોને': 5.0371489736808964e-05, 'દ્વારો': 0.0005037148973680896, 'દ્વારનું': 0.00027704319355244933, 'દ્વારમાં': 0.0005792721319733031, 'દ્વારને': 0.0004029719178944717, 'દ્વારમાંથી': 0.00020148595894723585, 'દ્વારિ': 0.00010074297947361793, 'દ્વારમાર્ગ': 5.0371489736808964e-05, 'દ્વારા.': 2.5185744868404482e-05, 'દ્વારથી': 0.0003274146832892583, 'દ્વારામતિ': 2.5185744868404482e-05, 'દ્વારાએ': 0.00010074297947361793, 'દ્વારોની': 0.00010074297947361793, 'દ્વારના': 0.00027704319355244933, 'દ્વારવાળા': 0.0001511144692104269, 'દ્વારેથી': 0.00010074297947361793, 'દ્વારી': 5.0371489736808964e-05, 'દ્વારાતિ': 2.5185744868404482e-05, 'દ્વારનો': 0.0002518574486840448, 'દ્વારાના': 0.00010074297947361793, 'દ્વારયુગ્મ.': 2.5185744868404482e-05, 'દ્વારવાળું': 0.0001511144692104269, 'દ્વારાજ': 7.555723460521345e-05, 'દ્વાર.': 2.5185744868404482e-05, 'દ્વારવાળી': 5.0371489736808964e-05, 'દ્વારાસમાજકાર્ય': 2.5185744868404482e-05, 'દ્વારભાગ': 2.5185744868404482e-05, 'દ્વારાની': 5.0371489736808964e-05, 'દ્વારવાળો': 7.555723460521345e-05, 'દ્વારામાં': 2.5185744868404482e-05, 'દ્વારોં': 5.0371489736808964e-05}\n",
            "પ્રાપ્ત {'પ્રાપ્ત': 0.8641639911089157, 'પ્રાપ્તિ': 0.132254877747592, 'પ્રાપ્તા': 0.0003704618424302297, 'પ્રાપ્તી': 0.0012348728081007657, 'પ્રાપ્તણ': 0.0006174364040503828, 'પ્રાપ્તીની': 0.0002469745616201531, 'પ્રાપ્તલ': 0.00012348728081007656, 'પ્રાપ્તિ.': 0.00012348728081007656, 'પ્રાપ્તિફલ': 0.00012348728081007656, 'પ્રાપ્તિસમ.': 0.00012348728081007656, 'પ્રાપ્ત.': 0.00012348728081007656, 'પ્રાપ્તવય': 0.00012348728081007656, 'પ્રાપ્તિનો.': 0.00012348728081007656, 'પ્રાપ્તિઆ': 0.00012348728081007656, 'પ્રાપ્ત્': 0.00012348728081007656}\n",
            "વિગત {'વિગત': 0.2784003091190108, 'વિગતો': 0.6093508500772797, 'વિગતે': 0.044435857805255025, 'વિગતોની': 0.026468315301391036, 'વિગતિ': 0.0003863987635239567, 'વિગતમાં': 0.006955177743431221, 'વિગતોથી': 0.0028979907264296756, 'વિગતથી': 0.00231839258114374, 'વિગતોમાં': 0.0028979907264296756, 'વિગતોના': 0.004250386398763524, 'વિગતની': 0.0032843894899536323, 'વિગતોને': 0.00463678516228748, 'વિગતોનો': 0.002704791344667697, 'વિગતેસમય': 0.00019319938176197836, 'વિગત.': 0.00019319938176197836, 'વિગતદિપેશભાઇ': 0.00019319938176197836, 'વિગતોનું': 0.002125193199381762, 'વિગતનું': 0.0015455950540958269, 'વિગતવર્ષ': 0.00019319938176197836, 'વિગતપૂર્ણ': 0.000579598145285935, 'વિગતઃ': 0.00019319938176197836, 'વિગતના': 0.0007727975270479134, 'વિગતને': 0.0009659969088098918, 'વિગતનો': 0.0009659969088098918, 'વિગતકાલ': 0.00019319938176197836, 'વિગતે.': 0.00019319938176197836, 'વિગતવાળી': 0.0003863987635239567, 'વિગતમાંનો': 0.0003863987635239567, 'વિગતવાળું': 0.0013523956723338485, 'વિગતોમુજબ': 0.00019319938176197836, 'વિગતવાળો': 0.0003863987635239567}\n",
            "મુજ {'મુજબ': 0.9755207541432264, 'મુજ': 0.01680097308803406, 'મુજે': 0.001672495058537327, 'મુજુકેશાનંદ.': 7.60225026607876e-05, 'મુજાજ': 7.60225026607876e-05, 'મુજીબ': 0.0005321575186255132, 'મુજવણ': 7.60225026607876e-05, 'મુજથી': 0.001140337539911814, 'મુજહ': 0.000380112513303938, 'મુજબબૂ': 7.60225026607876e-05, 'મુજૂબ': 7.60225026607876e-05, 'મુજમાં': 0.0017485175611981147, 'મુજમ્મિલ': 0.00022806750798236278, 'મુજબ.': 7.60225026607876e-05, 'મુજ્બ': 7.60225026607876e-05, 'મુજી': 0.00022806750798236278, 'મુજહબ': 7.60225026607876e-05, 'મુજબઃ': 0.0001520450053215752, 'મુજુકેશાનંદ': 7.60225026607876e-05, 'મુજનિ': 0.0001520450053215752, 'મુજનો': 7.60225026607876e-05, 'મુજ્જાતિ': 7.60225026607876e-05, 'મુજમ': 7.60225026607876e-05, 'મુજના': 7.60225026607876e-05, 'મુજમમીલ': 7.60225026607876e-05, 'મુજાહીદ': 7.60225026607876e-05, 'મુજમ્મીલ': 7.60225026607876e-05, 'મુજવ': 7.60225026607876e-05, 'મુજબસ': 7.60225026607876e-05, 'મુજૂ': 7.60225026607876e-05}\n",
            "દાં {'દાંતા': 0.12366474938373048, 'દાંત': 0.7875924404272802, 'દાંડ': 0.019309778142974528, 'દાંડીપત્ર.': 0.0004108463434675431, 'દાંડો.': 0.0004108463434675431, 'દાંન': 0.0008216926869350862, 'દાંડીવાડ': 0.0004108463434675431, 'દાંતો': 0.037387017255546426, 'દાંડીકૂચ': 0.004108463434675432, 'દાંતણ': 0.005751848808545604, 'દાંનાભાઇ': 0.0016433853738701725, 'દાંતવાળાં.': 0.0004108463434675431, 'દાંતો.': 0.0004108463434675431, 'દાં': 0.0004108463434675431, 'દાંવ': 0.0008216926869350862, 'દાંતીવાડ': 0.0008216926869350862, 'દાંડીઘર.': 0.0004108463434675431, 'દાંસ': 0.0008216926869350862, 'દાંતિ': 0.0024650780608052587, 'દાંથ': 0.0008216926869350862, 'દાંમ્પત્ય': 0.0004108463434675431, 'દાંત.': 0.0004108463434675431, 'દાંડીઃ': 0.0008216926869350862, 'દાંતવાળું.': 0.0004108463434675431, 'દાંગાઈ': 0.0004108463434675431, 'દાંડી.': 0.0004108463434675431, 'દાંડીકુચ': 0.0008216926869350862, 'દાંતાદાર.': 0.0004108463434675431, 'દાંડલું.': 0.0004108463434675431, 'દાંગ': 0.0012325390304026294, 'દાંનાભાઈ': 0.0004108463434675431, 'દાંડાઇ': 0.0004108463434675431, 'દાંતરૂં': 0.0004108463434675431, 'દાંતીઆ': 0.0008216926869350862, 'દાંતી.': 0.0004108463434675431, 'દાંડીમાર્ગ': 0.0008216926869350862, 'દાંતનું.': 0.0004108463434675431, 'દાંડીદાર.': 0.0004108463434675431, 'દાંડીયારાસ': 0.0004108463434675431, 'દાંતરડું.': 0.0004108463434675431, 'દાં.': 0.0004108463434675431}\n",
            "નગર {'નગરમાં': 0.14140027456363993, 'નગરને': 0.013139831339478329, 'નગરી': 0.14747989801921946, 'નગરના': 0.06805255932535792, 'નગર': 0.40223573249656797, 'નગરની': 0.03608550696214944, 'નગરોમાં': 0.005491272798587958, 'નગરા': 0.0009805844283192783, 'નગરીને': 0.0345165718768386, 'નગરનું': 0.019023337909394, 'નગરીઓ': 0.003333987056285546, 'નગરમાંં': 0.00019611688566385565, 'નગરીનું': 0.010001961168856639, 'નગરીના': 0.009217493626201216, 'નગરાજ': 0.00019611688566385565, 'નગરીમાં': 0.030986467934889194, 'નગરીનો': 0.012551480682486762, 'નગરાજ્ય': 0.0003922337713277113, 'નગરો': 0.007452441655226515, 'નગરનો': 0.018238870366738578, 'નગરોની': 0.0013728181996469895, 'નગરથી': 0.006275740341243381, 'નગરમાંથી': 0.002353402627966268, 'નગરું': 0.0009805844283192783, 'નગરપતિ': 0.004706805255932536, 'નગરીની': 0.004706805255932536, 'નગરીએ': 0.0009805844283192783, 'નગરોને': 0.0007844675426554226, 'નગરોનું': 0.00019611688566385565, 'નગરવધૂ': 0.0007844675426554226, 'નગરોના': 0.0015689350853108452, 'નગરે': 0.000588350656991567, 'નગરસંઘ': 0.0003922337713277113, 'નગરમાંહિ': 0.0003922337713277113, 'નગરણ': 0.0003922337713277113, 'નગરીનાં': 0.0009805844283192783, 'નગરતુલ્ય': 0.0003922337713277113, 'નગર,અમદાવાદ.': 0.00019611688566385565, 'નગરાસણ': 0.0007844675426554226, 'નગરોમા': 0.00019611688566385565, 'નગરીમેં': 0.00019611688566385565, 'નગરોનાં': 0.00019611688566385565, 'નગરાં': 0.0003922337713277113, 'નગરમાંનું': 0.000588350656991567, 'નગરનાં': 0.0021572857423024124, 'નગરોનો': 0.0003922337713277113, 'નગરાદિ': 0.00019611688566385565, 'નગરપરિભ્રમણ': 0.00019611688566385565, 'નગરઈ': 0.0003922337713277113, 'નગરમાંનો': 0.00019611688566385565, 'નગરઇં': 0.0003922337713277113, 'નગરનું.': 0.00019611688566385565, 'નગરીય': 0.00019611688566385565, 'નગરમ': 0.0003922337713277113, 'નગરોમાંથી': 0.00019611688566385565, 'નગરમૂલ.': 0.00019611688566385565, 'નગરૂં': 0.00019611688566385565, 'નગરકોષાધ્યક્ષ': 0.00019611688566385565, 'નગરી.': 0.00019611688566385565, 'નગરુ': 0.0003922337713277113, 'નગરમેં': 0.00019611688566385565, 'નગરેથી': 0.00019611688566385565, 'નગરપાદુકા.': 0.00019611688566385565, 'નગરભૂષણ': 0.0003922337713277113, 'નગરપ્રમુખ': 0.0003922337713277113, 'નગરક્ષેત્ર.': 0.00019611688566385565, 'નગરવસવાટ': 0.00019611688566385565, 'નગર.': 0.00019611688566385565, 'નગરોટ': 0.00019611688566385565, 'નગરપતિ.': 0.00019611688566385565, 'નગરોળ': 0.00019611688566385565}\n",
            "પ્રવેશ {'પ્રવેશવાના': 0.0038267258533598654, 'પ્રવેશ': 0.7045767641206184, 'પ્રવેશી': 0.04209398438695852, 'પ્રવેશક': 0.004439001989897444, 'પ્રવેશતાં': 0.011786315628348386, 'પ્રવેશો': 0.06229909689269861, 'પ્રવેશવા': 0.02617480483698148, 'પ્રવેશથી': 0.005051278126435022, 'પ્રવેશને': 0.0035205877850910763, 'પ્રવેશવાની': 0.007194244604316547, 'પ્રવેશની': 0.022654217051890402, 'પ્રવેશનાં': 0.00030613806826878924, 'પ્રવેશવાનો': 0.0035205877850910763, 'પ્રવેશતા': 0.023419562222562376, 'પ્રવેશવું': 0.0032144497168222867, 'પ્રવેશીને': 0.010867901423542017, 'પ્રવેશે': 0.017449869891320986, 'પ્રવેશતાજ': 0.007959589774988519, 'પ્રવેશ્ય': 0.00045920710240318386, 'પ્રવેશનો': 0.0064288994336445734, 'પ્રવેશશે': 0.001224552273075157, 'પ્રવેશમાં': 0.008265727843257308, 'પ્રવેશનાર': 0.0009184142048063677, 'પ્રવેશના': 0.005510485228838206, 'પ્રવેશેલ': 0.000765345170671973, 'પ્રવેશવાનું': 0.0026021735802847085, 'પ્રવેશનું': 0.00045920710240318386, 'પ્રવેશમાર્ગ': 0.001224552273075157, 'પ્રવેશનિષેધ': 0.004439001989897444, 'પ્રવેશતો': 0.0016837593754783406, 'પ્રવેશન': 0.00015306903413439462, 'પ્રવેશિનાંગુહાપાલ': 0.00030613806826878924, 'પ્રવેશનારને': 0.00015306903413439462, 'પ્રવેશવાને': 0.00015306903413439462, 'પ્રવેશેલી': 0.0006122761365375785, 'પ્રવેશ્યો.': 0.00015306903413439462, 'પ્રવેશવિ': 0.00015306903413439462, 'પ્રવેશખંડ': 0.0006122761365375785, 'પ્રવેશષ': 0.00015306903413439462, 'પ્રવેશવાથી': 0.0006122761365375785, 'પ્રવેશ.': 0.00015306903413439462, 'પ્રવેશબંધિ': 0.00015306903413439462, 'પ્રવેશક.': 0.00015306903413439462, 'પ્રવેશવવા': 0.00030613806826878924, 'પ્રવેશવામાં': 0.000765345170671973, 'પ્રવેશીએ': 0.00015306903413439462, 'પ્રવેશું': 0.00015306903413439462, 'પ્રવેશેલો': 0.00030613806826878924, 'પ્રવેશબંધ': 0.00015306903413439462, 'પ્રવેશસ્થાન.': 0.00015306903413439462, 'પ્રવેશપાસ': 0.00015306903413439462}\n",
            "મુખ્ય {'મુખ્ય': 0.9919083969465649, 'મુખ્યા': 0.0003816793893129771, 'મુખ્યને': 0.0004580152671755725, 'મુખ્યત': 0.0011450381679389313, 'મુખ્યરોડ': 7.633587786259542e-05, 'મુખ્યનો': 0.0003053435114503817, 'મુખ્યમાંથી': 7.633587786259542e-05, 'મુખ્યમાર્ગ': 0.0015267175572519084, 'મુખ્યતઃ': 0.001068702290076336, 'મુખ્યાના': 7.633587786259542e-05, 'મુખ્યાઓ': 7.633587786259542e-05, 'મુખ્યમાં': 0.0003816793893129771, 'મુખ્યવેદી.': 7.633587786259542e-05, 'મુખ્યસ્તંભ': 7.633587786259542e-05, 'મુખ્યશ': 7.633587786259542e-05, 'મુખ્યશરાફ.': 7.633587786259542e-05, 'મુખ્યકારણ': 0.00015267175572519084, 'મુખ્યબંધ': 7.633587786259542e-05, 'મુખ્યની': 0.0003053435114503817, 'મુખ્યમ': 0.00022900763358778625, 'મુખ્યપદ.': 7.633587786259542e-05, 'મુખ્યભૂમિ': 0.00015267175572519084, 'મુખ્યગિરિ': 0.00015267175572519084, 'મુખ્યથી': 0.00022900763358778625, 'મુખ્ય.': 7.633587786259542e-05, 'મુખ્યભેદ': 0.00015267175572519084, 'મુખ્યના': 0.00022900763358778625, 'મુખ્યછ': 7.633587786259542e-05, 'મુખ્યગેટ': 7.633587786259542e-05, 'મુખ્યસેનાપતિ': 7.633587786259542e-05, 'મુખ્યભાગ.': 7.633587786259542e-05, 'મુખ્યુ': 7.633587786259542e-05}\n",
            "બજા {'બજાર': 0.8771144278606965, 'બજાજ': 0.08109452736318408, 'બજાઈ': 0.001990049751243781, 'બજારવાદ': 0.0009950248756218905, 'બજારમૂલ્ય': 0.0029850746268656717, 'બજારૂ': 0.005970149253731343, 'બજાવેલ': 0.005970149253731343, 'બજાણીયાવાસ': 0.0004975124378109452, 'બજારવર્ગ': 0.0004975124378109452, 'બજારમેં': 0.0004975124378109452, 'બજાવતાદશરથસિંહ': 0.001990049751243781, 'બજાત': 0.0004975124378109452, 'બજારવાળું.': 0.0004975124378109452, 'બજાની': 0.0009950248756218905, 'બજારમા઼': 0.0009950248756218905, 'બજાખ': 0.0009950248756218905, 'બજાણિયણ': 0.0004975124378109452, 'બજાવું': 0.0024875621890547263, 'બજારભચ્છ': 0.0004975124378109452, 'બજાવનારૂં': 0.0009950248756218905, 'બજાવાઈ': 0.0004975124378109452, 'બજાનો': 0.0004975124378109452, 'બજામાં': 0.0004975124378109452, 'બજાવનાર.': 0.0004975124378109452, 'બજાજ.': 0.0004975124378109452, 'બજારણ': 0.0009950248756218905, 'બજાવ': 0.0009950248756218905, 'બજા': 0.0004975124378109452, 'બજાજવિકાસ': 0.0004975124378109452, 'બજાવાનું': 0.0009950248756218905, 'બજાવતો.': 0.0004975124378109452, 'બજારનું.': 0.0004975124378109452, 'બજારૂં': 0.0004975124378109452, 'બજાવા': 0.0009950248756218905, 'બજાર.': 0.0004975124378109452, 'બજાક': 0.0004975124378109452, 'બજાથી': 0.0004975124378109452, 'બજાવશેબાયડ': 0.0004975124378109452, 'બજાઉં': 0.0004975124378109452, 'બજાએ': 0.0004975124378109452}\n",
            "અને {'અને': 0.9710101991469487, 'અનેક': 0.028793747195343507, 'અનેસત્ય': 4.356747949060903e-06, 'અને્': 8.713495898121806e-06, 'અનેજ': 4.356747949060903e-06, 'અનેય': 1.307024384718271e-05, 'અનેકવિદ્ય': 2.1783739745304514e-06, 'અનેકાન્ત્વાદ': 2.1783739745304514e-06, 'અનેે': 1.524861782171316e-05, 'અનેટ્રિચોમોનીયાસીસ': 2.1783739745304514e-06, 'અનેઙ્ગ': 1.0891869872652257e-05, 'અનેત': 1.307024384718271e-05, 'અનેવ': 4.356747949060903e-06, 'અનેકવિઘ': 2.1783739745304514e-06, 'અનેકાંગ': 2.1783739745304514e-06, 'અનેરૂ': 3.049723564342632e-05, 'અનેઆજ': 2.1783739745304514e-06, 'અનેર': 2.1783739745304514e-06, 'અનેબલ્ડ': 4.356747949060903e-06, 'અનેકઘાસ': 2.1783739745304514e-06, 'અનેકસંખ્ય': 2.1783739745304514e-06, 'અનેકિ': 4.356747949060903e-06, 'અનેલ': 2.1783739745304514e-06, 'અને.': 3.049723564342632e-05, 'અનેન': 4.356747949060903e-06, 'અનેક.': 2.1783739745304514e-06, 'અનેલાર્ડ': 4.356747949060903e-06, 'અનેં': 4.356747949060903e-06, 'અનેકદેવવિવાદ': 2.1783739745304514e-06, 'અનેડ': 2.1783739745304514e-06, 'અનેબી.': 2.1783739745304514e-06, 'અનેકપણું.': 2.1783739745304514e-06, 'અનેસીએઓફ': 2.1783739745304514e-06, 'અનેાં': 2.1783739745304514e-06, 'અનેભારતીય': 2.1783739745304514e-06}\n",
            "ઝળહળ {'ઝળહળતા': 0.10204081632653061, 'ઝળહળા': 0.14285714285714285, 'ઝળહળે': 0.11224489795918367, 'ઝળહળતાં': 0.10204081632653061, 'ઝળહળ': 0.05102040816326531, 'ઝળહળી': 0.16326530612244897, 'ઝળહળતો': 0.10204081632653061, 'ઝળહળવું': 0.1326530612244898, 'ઝળહળ.': 0.01020408163265306, 'ઝળહળુ': 0.01020408163265306, 'ઝળહળવા': 0.01020408163265306, 'ઝળહળતું.': 0.01020408163265306, 'ઝળહળશે': 0.01020408163265306, 'ઝળહળું': 0.01020408163265306, 'ઝળહળવાની': 0.02040816326530612, 'ઝળહળવું.': 0.01020408163265306}\n",
            "પ્રકાશિ {'પ્રકાશિત': 0.9995608256477821, 'પ્રકાશિતા': 0.0004391743522178305}\n",
            "માર્ગ {'માર્ગ': 0.6020131192038001, 'માર્ગે': 0.15301967880570005, 'માર્ગમાં': 0.0490839176656865, 'માર્ગોથી': 0.001130965844831486, 'માર્ગો': 0.0772449672019905, 'માર્ગનો': 0.009387016512101335, 'માર્ગમાંથી': 0.004297670210359647, 'માર્ગોની': 0.004297670210359647, 'માર્ગોને': 0.004410766794842796, 'માર્ગને': 0.017982356932820628, 'માર્ગનું': 0.00972630626555078, 'માર્ગથી': 0.008934630174168741, 'માર્ગીય': 0.006785795068988917, 'માર્ગની': 0.01617281158109025, 'માર્ગોમાં': 0.001470255598280932, 'માર્ગના': 0.00983940285003393, 'માર્ગેથી': 0.0028274146120787153, 'માર્ગી': 0.004410766794842796, 'માર્ગારેટ': 0.00045238633793259444, 'માર્ગેઃ': 0.00011309658448314861, 'માર્ગોના': 0.002035738520696675, 'માર્ગોનું': 0.002261931689662972, 'માર્ગમાંનું': 0.00045238633793259444, 'માર્ગોનાં': 0.00022619316896629722, 'માર્ગનાં': 0.001470255598280932, 'માર્ગપદ્ધતિ': 0.00011309658448314861, 'માર્ગવિભાજક.': 0.00011309658448314861, 'માર્ગા': 0.00022619316896629722, 'માર્ગોનો': 0.00045238633793259444, 'માર્ગિય': 0.0012440624293146346, 'માર્ગવાળું': 0.0006785795068988917, 'માર્ગશીષ્': 0.00022619316896629722, 'માર્ગણ': 0.0016964487672472292, 'માર્ગઃ': 0.00022619316896629722, 'માર્ગબર્ગ': 0.00011309658448314861, 'માર્ગચ્યૂતિ': 0.00011309658448314861, 'માર્ગીઓનું': 0.00022619316896629722, 'માર્ગવાળા': 0.0006785795068988917, 'માર્ગીમાં': 0.00011309658448314861, 'માર્ગચ્યુતિ': 0.0006785795068988917, 'માર્ગહશીષોંહ': 0.00022619316896629722, 'માર્ગનિર્ધારણ': 0.00033928975344944584, 'માર્ગવાળી': 0.000565482922415743, 'માર્ગભ્રષ્ટ.': 0.00011309658448314861, 'માર્ગીકરણ': 0.00022619316896629722, 'માર્ગમાંનો': 0.0006785795068988917, 'માર્ગેં': 0.00022619316896629722, 'માર્ગવાદ': 0.00011309658448314861, 'માર્ગ.': 0.00011309658448314861, 'માર્ગોમાંથી': 0.00011309658448314861, 'માર્ગભ્રષ્ટત્તા.': 0.00011309658448314861, 'માર્ગે.': 0.00011309658448314861, 'માર્ગદોડ': 0.00011309658448314861}\n",
            "ધનતેરસ {'ધનતેરસની': 0.010101010101010102, 'ધનતેરસ': 0.23232323232323232, 'ધનતેરસના': 0.20202020202020202, 'ધનતેરસનો': 0.020202020202020204, 'ધનતેરસે': 0.04040404040404041, 'ધનતેરસથી': 0.4444444444444444, 'ધનતેરસને': 0.04040404040404041, 'ધનતેરસ.': 0.010101010101010102}\n",
            "ઢળત {'ઢળતી': 0.4024390243902439, 'ઢળતો': 0.12804878048780488, 'ઢળતા': 0.14634146341463414, 'ઢળતું': 0.25914634146341464, 'ઢળતાની': 0.012195121951219513, 'ઢળતાં': 0.04878048780487805, 'ઢળતું.': 0.003048780487804878}\n",
            "રાત્ર {'રાત્રીના': 0.08949207202364956, 'રાત્રિ': 0.09352324643912927, 'રાત્રે': 0.7367643106691749, 'રાત્રીનિવાસ': 0.0001343724805159903, 'રાત્રિનિવાસ': 0.0002687449610319806, 'રાત્ર': 0.0008062348830959419, 'રાત્રીએ': 0.010481053480247245, 'રાત્રી': 0.04958344531040043, 'રાત્રીનાં': 0.00403117441547971, 'રાત્રીનો': 0.0014780972856758936, 'રાત્રીથી': 0.002553077129803816, 'રાત્રિરોકાણ': 0.002553077129803816, 'રાત્રીમાં': 0.0005374899220639613, 'રાત્રીની': 0.002015587207739855, 'રાત્રે.': 0.00040311744154797097, 'રાત્રીનું': 0.001209352324643913, 'રાત્રિ.': 0.0001343724805159903, 'રાત્રેની': 0.0002687449610319806, 'રાત્રેથી': 0.00040311744154797097, 'રાત્રિયુદ્ધ': 0.00040311744154797097, 'રાત્રો': 0.0001343724805159903, 'રાત્રીરોકાણ': 0.00040311744154797097, 'રાત્રિરોકણ': 0.0002687449610319806, 'રાત્રીઓ': 0.0001343724805159903, 'રાત્રા': 0.0001343724805159903, 'રાત્રિદીઠ': 0.0001343724805159903, 'રાત્રી.': 0.0001343724805159903, 'રાત્રીને': 0.0005374899220639613, 'રાત્રિયુદ્ધ.': 0.0001343724805159903, 'રાત્રીનાં.': 0.0001343724805159903, 'રાત્રનો': 0.0002687449610319806, 'રાત્રિનું.': 0.0001343724805159903, 'રાત્રેના': 0.0001343724805159903, 'રાત્રના': 0.0001343724805159903, 'રાત્રિના.': 0.0001343724805159903}\n",
            "સુમાર {'સુમારે': 0.9540372670807453, 'સુમારીની': 0.0012422360248447205, 'સુમાર': 0.03229813664596273, 'સુમારની': 0.0037267080745341614, 'સુમારભાઇ': 0.0012422360248447205, 'સુમારથી': 0.0037267080745341614, 'સુમારમાં': 0.0037267080745341614}\n",
            "જુદ {'જુદી': 0.26462098393574296, 'જુદો': 0.050514558232931724, 'જુદાં': 0.1507906626506024, 'જુદું': 0.09939759036144578, 'જુદા': 0.39602158634538154, 'જુદાઈ': 0.010918674698795181, 'જુદે': 0.02491214859437751, 'જુદી.': 6.275100401606425e-05, 'જુદુ': 0.0015060240963855422, 'જુદો.': 6.275100401606425e-05, 'જુદુંજુદું.': 6.275100401606425e-05, 'જુદેરૂ': 6.275100401606425e-05, 'જુદૂં': 0.000251004016064257, 'જુદોજ': 0.00018825301204819278, 'જુદુંજ': 0.0001255020080321285, 'જુદાને': 6.275100401606425e-05, 'જુદીથી': 0.0001255020080321285, 'જુદા.': 6.275100401606425e-05, 'જુદં': 6.275100401606425e-05, 'જુદાઈ.': 6.275100401606425e-05, 'જુદુ.': 6.275100401606425e-05, 'જુદું.': 6.275100401606425e-05}\n",
            "ચાર {'ચાર': 0.800853614239012, 'ચારેય': 0.03982019614965492, 'ચારમાંથી': 0.0032237559026516527, 'ચારા': 0.005130766436614602, 'ચારે': 0.08390846349436978, 'ચારો': 0.009171812568107519, 'ચારણ': 0.012531783508899382, 'ચારમાં': 0.0013621503814021069, 'ચારની': 0.0060388666908826735, 'ચારથી': 0.013803123864874683, 'ચારને': 0.0024064656738103886, 'ચારવું': 0.0004994551398474392, 'ચારનો': 0.002905920813657828, 'ચારમાંની': 0.0008626952415546677, 'ચારીને': 0.0002270250635670178, 'ચારેય.': 4.540501271340356e-05, 'ચારમાંનો': 0.0010897203051216855, 'ચારનું': 0.0016345804576825282, 'ચારીઈ': 9.081002542680712e-05, 'ચારાને': 0.00040864511442063206, 'ચારઠ': 0.00031783508899382493, 'ચારમાર્ગીય': 0.0008172902288412641, 'ચારેનો': 4.540501271340356e-05, 'ચારૂ': 0.0005448601525608427, 'ચારમાસ': 9.081002542680712e-05, 'ચારૂસેટ': 0.0032237559026516527, 'ચારનાં': 0.0004994551398474392, 'ચારી': 0.0005902651652742463, 'ચાર.': 4.540501271340356e-05, 'ચારખૂણા.': 4.540501271340356e-05, 'ચારાના': 0.0004540501271340356, 'ચારીત્ર્ય': 4.540501271340356e-05, 'ચાર્તુમાસ': 0.00018162005085361425, 'ચારેથી': 0.00018162005085361425, 'ચારતાં': 0.00040864511442063206, 'ચારુ': 0.0009535052669814748, 'ચારપાંચ': 0.0005448601525608427, 'ચારુસેટ': 0.00018162005085361425, 'ચારમાંનું': 0.00027243007628042137, 'ચારેની': 9.081002542680712e-05, 'ચારાની': 0.0004994551398474392, 'ચારોં': 0.00013621503814021069, 'ચારામાં': 0.0003632401017072285, 'ચારેલ': 0.0002270250635670178, 'ચારૂલ': 0.00013621503814021069, 'ચારણ.': 4.540501271340356e-05, 'ચારમાંના': 0.0002270250635670178, 'ચારેના': 0.0004540501271340356, 'ચાર્કોલ': 4.540501271340356e-05, 'ચારરસ્તાપાર્કિંગ': 4.540501271340356e-05, 'ચારલોટ': 4.540501271340356e-05, 'ચારત': 9.081002542680712e-05, 'ચાર્લોટ્ટ': 4.540501271340356e-05, 'ચારલોટ્ટ': 4.540501271340356e-05, 'ચારે.': 4.540501271340356e-05, 'ચારવાળો': 4.540501271340356e-05, 'ચારાનો': 0.00018162005085361425, 'ચારસ': 9.081002542680712e-05, 'ચારઈ': 9.081002542680712e-05, 'ચારેનું': 9.081002542680712e-05, 'ચાર્લસ': 4.540501271340356e-05, 'ચારામાંથી': 9.081002542680712e-05, 'ચારુલ': 9.081002542680712e-05, 'ચારેંદ્રિય': 9.081002542680712e-05, 'ચારેમાસ': 4.540501271340356e-05, 'ચારીએ': 9.081002542680712e-05, 'ચારઈંચ': 4.540501271340356e-05, 'ચારૂં': 9.081002542680712e-05, 'ચારઆની.': 4.540501271340356e-05, 'ચારો.': 4.540501271340356e-05, 'ચારજ': 9.081002542680712e-05, 'ચારૂસેટની.': 4.540501271340356e-05, 'ચાર્લેસ': 4.540501271340356e-05, 'ચારબેલ': 9.081002542680712e-05, 'ચારણીદુહો.': 4.540501271340356e-05, 'ચારું': 9.081002542680712e-05, 'ચારાથી': 4.540501271340356e-05, 'ચારણો.': 4.540501271340356e-05, 'ચારતો': 4.540501271340356e-05, 'ચારણી.': 4.540501271340356e-05, 'ચારિથંગ': 4.540501271340356e-05, 'ચારૂસેટ.': 4.540501271340356e-05, 'ચારનારની': 4.540501271340356e-05, 'ચારોતરફ': 4.540501271340356e-05, 'ચારટ': 4.540501271340356e-05}\n",
            "દુકાન {'દુકાનોને': 0.023294908741594622, 'દુકાનનું': 0.014169068203650336, 'દુકાનમાં': 0.13640730067243034, 'દુકાનના': 0.04562920268972142, 'દુકાનો': 0.15922190201729106, 'દુકાન': 0.3184438040345821, 'દુકાનોમાં': 0.06027857829010567, 'દુકાને': 0.08117195004803074, 'દુકાનનાં': 0.0024015369836695487, 'દુકાનવાળા': 0.0004803073967339097, 'દુકાનોના': 0.01681075888568684, 'દુકાનની': 0.02473583093179635, 'દુકાનોની': 0.014169068203650336, 'દુકાનોમાંથી': 0.006964457252641691, 'દુકાનમાંથી': 0.04346781940441883, 'દુકાનોનું': 0.004322766570605188, 'દુકાનનો': 0.011287223823246878, 'દુકાનને': 0.011767531219980788, 'દુકાનોમા': 0.0007204610951008645, 'દુકાનોનો': 0.002161383285302594, 'દુકાનેથી': 0.007444764649375601, 'દુકાનબંધ': 0.00024015369836695484, 'દુકાનોનાં': 0.006003842459173871, 'દુકાનોએ': 0.0009606147934678194, 'દુકાના': 0.0009606147934678194, 'દુકાનથી': 0.0016810758885686839, 'દુકાનકાર': 0.0004803073967339097, 'દુકાનોથી': 0.0004803073967339097, 'દુકાનનં': 0.0004803073967339097, 'દુકાનવાળાઓને': 0.00024015369836695484, 'દુકાનાના': 0.0004803073967339097, 'દુકાનમાંની': 0.0007204610951008645, 'દુકાનવાળાઓ': 0.00024015369836695484, 'દુકાની': 0.00024015369836695484, 'દુકાનીની': 0.00024015369836695484, 'દુકાન.': 0.00024015369836695484, 'દુકાનીના': 0.00024015369836695484, 'દુકાનોમાંથી.': 0.00024015369836695484, 'દુકાનસર્વિસ': 0.00024015369836695484, 'દુકાનનું.': 0.00024015369836695484}\n",
            "નિશાન {'નિશાન': 0.5009808729769495, 'નિશાની': 0.35850907307503677, 'નિશાનો': 0.02918097106424718, 'નિશાનમાં': 0.013241785188818049, 'નિશાનથી': 0.003678273663560569, 'નિશાનીમાં': 0.005149583128984796, 'નિશાનીઓ': 0.018391368317802845, 'નિશાને': 0.016429622363903874, 'નિશાનીની': 0.004413928396272682, 'નિશાનની': 0.0029426189308484553, 'નિશાનવાળી': 0.0007356547327121138, 'નિશાના': 0.010789602746444336, 'નિશાની.': 0.0002452182442373713, 'નિશાનીએ': 0.0024521824423737125, 'નિશાનીને': 0.0012260912211868563, 'નિશાનીવાળી': 0.0034330554193231977, 'નિશાનનું': 0.0007356547327121138, 'નિશાનીફાંદ': 0.0002452182442373713, 'નિશાનને': 0.004413928396272682, 'નિશાનવાળો': 0.00196174595389897, 'નિશાનું': 0.0004904364884747426, 'નિશાનીનો': 0.004168710152035311, 'નિશાનેબાજ': 0.000980872976949485, 'નિશાનના': 0.000980872976949485, 'નિશાનવાળા': 0.000980872976949485, 'નિશાનાને': 0.0002452182442373713, 'નિશાનીઓનો': 0.0004904364884747426, 'નિશાનીઓની': 0.000980872976949485, 'નિશાનનો': 0.000980872976949485, 'નિશાનીથી': 0.004659146640510054, 'નિશાનીઓને': 0.0002452182442373713, 'નિશાનોમાંથી': 0.0002452182442373713, 'નિશાનોને': 0.0002452182442373713, 'નિશાનવાળું': 0.0014713094654242277, 'નિશાનવાળું.': 0.0002452182442373713, 'નિશાનીવાળું.': 0.0002452182442373713, 'નિશાનીનું': 0.000980872976949485, 'નિશાનીના': 0.0004904364884747426, 'નિશાનીઓ.': 0.0002452182442373713, 'નિશાનધ્વનિ': 0.0004904364884747426, 'નિશાનનાં': 0.0002452182442373713, 'નિશાન.': 0.0002452182442373713, 'નિશાનીઓના': 0.0002452182442373713, 'નિશાનવાળો.': 0.0002452182442373713}\n",
            "બનાવ {'બનાવી': 0.28348715986801204, 'બનાવની': 0.032757878628473056, 'બનાવોમાં': 0.0029649466787815023, 'બનાવીને': 0.06274209746066664, 'બનાવમાં': 0.021854526325857205, 'બનાવેલો': 0.040122423604801304, 'બનાવવાની': 0.08780067906843288, 'બનાવો': 0.07359762804265697, 'બનાવે': 0.09416096791162545, 'બનાવતા': 0.021806704605231698, 'બનાવશે': 0.013820477260771843, 'બનાવવાના': 0.02625412462340395, 'બનાવવું': 0.07890583903208838, 'બનાવાશે': 0.005977715078188514, 'બનાવવાથી': 0.022571852135239826, 'બનાવીએ': 0.003634450767538616, 'બનાવોએ': 0.00019128688250203243, 'બનાવવાનું': 0.035148964659748455, 'બનાવોની': 0.002295442590024389, 'બનાવવાનો': 0.032470948304720006, 'બનાવોનો': 0.0007173258093826216, 'બનાવથી': 0.005834249916311989, 'બનાવને': 0.007794940461957821, 'બનાવોને': 0.0020563339868968486, 'બનાવાયેલી': 0.004447420018172254, 'બનાવતાં': 0.009994739610731194, 'બનાવના': 0.007460188417579264, 'બનાવનો': 0.0014346516187652431, 'બનાવશો': 0.0055951413131844485, 'બનાવોનું': 0.0011477212950121946, 'બનાવનાં': 0.0010998995743866865, 'બનાવતો': 0.0052125675481803834, 'બનાવનું': 0.0016737602218927837, 'બનાવવ': 0.00038257376500406485, 'બનાવોના': 0.0008129692506336378, 'બનાવત': 0.00014346516187652432, 'બનાવવાળી': 0.00014346516187652432, 'બનાવોથી': 0.0007651475300081297, 'બનાવામાંથી': 4.782172062550811e-05, 'બનાવોનાં': 9.564344125101621e-05, 'બનાવવાળો': 9.564344125101621e-05, 'બનાવમાંથી': 9.564344125101621e-05, 'બનાવમાંનો': 9.564344125101621e-05, 'બનાવુ': 0.00014346516187652432, 'બનાવીનો': 4.782172062550811e-05, 'બનાવોમાંના': 4.782172062550811e-05, 'બનાવીના': 4.782172062550811e-05}\n",
            "હત {'હતી': 0.34090007037532394, 'હતો': 0.218930628608403, 'હતું': 0.18427995370571107, 'હતાં': 0.039540259021215995, 'હતા': 0.20229403544114655, 'હતુ': 0.012357373232695508, 'હતોકઈ': 3.335323409634415e-06, 'હત': 0.00014341890661427985, 'હતોઆ': 2.668258727707532e-05, 'હતાઃ': 0.0001100656725179357, 'હતુઃ': 4.669452773488181e-05, 'હતનોરાઘાટ': 6.67064681926883e-06, 'હતીઃ': 0.0004002388091561298, 'હતા.': 3.0017910686709736e-05, 'હતા઼': 1.0005970228903246e-05, 'હતીડાન્સ': 3.335323409634415e-06, 'હતીનવોદય': 3.335323409634415e-06, 'હતોભોગ': 3.335323409634415e-06, 'હતીય': 6.67064681926883e-06, 'હતુ.': 3.335323409634415e-06, 'હતું.': 4.33592043252474e-05, 'હતીઆરોગ્ય': 3.335323409634415e-06, 'હતોને': 1.0005970228903246e-05, 'હતાઆ': 4.0023880915612983e-05, 'હતી.': 5.670049796378506e-05, 'હતોઃ': 3.668855750597857e-05, 'હતાથરાદ': 3.335323409634415e-06, 'હતીઅગાઉ': 1.0005970228903246e-05, 'હતીને': 1.334129363853766e-05, 'હતુસાંસદ': 3.335323409634415e-06, 'હતોએ': 6.67064681926883e-06, 'હતીએ': 2.3347263867440906e-05, 'હતીફરિયાદ': 2.0011940457806492e-05, 'હતીં': 6.67064681926883e-06, 'હતોલાલ': 3.335323409634415e-06, 'હતીચિરિપાલ': 3.335323409634415e-06, 'હતોસિવિલ': 6.67064681926883e-06, 'હતાઅબિય': 3.335323409634415e-06, 'હતાય': 1.0005970228903246e-05, 'હતે': 4.33592043252474e-05, 'હતોયસ': 3.335323409634415e-06, 'હતીવર્ષ': 6.67064681926883e-06, 'હતીઆણંદ': 3.335323409634415e-06, 'હતુ઼': 1.6676617048172077e-05, 'હતોજસદણ': 3.335323409634415e-06, 'હતીઅક્ષય': 3.335323409634415e-06, 'હતો.': 8.004776183122597e-05, 'હતંદ': 3.335323409634415e-06, 'હતાવાળ': 3.335323409634415e-06, 'હતાએ': 6.67064681926883e-06, 'હતં': 2.0011940457806492e-05, 'હતૂ': 1.0005970228903246e-05, 'હતાબાબુલાલ': 3.335323409634415e-06, 'હતીપણ': 1.0005970228903246e-05, 'હતીખાસ': 3.335323409634415e-06, 'હતીઅમદાવાદ': 6.67064681926883e-06, 'હતોપણ': 3.335323409634415e-06, 'હતાપીઠાઇ': 3.335323409634415e-06, 'હતુ઼ં': 6.67064681926883e-06, 'હતીનડિયાદ': 3.335323409634415e-06, 'હતુંઉલ્લેખનિય': 6.67064681926883e-06, 'હતોયકોંગ્રેસ': 3.335323409634415e-06, 'હતુંપોલીસ': 3.335323409634415e-06, 'હતુઁ': 3.335323409634415e-06, 'હતીજસદણ': 3.335323409634415e-06, 'હતુંદક્ષિણ': 3.335323409634415e-06, 'હતીઆ': 3.668855750597857e-05, 'હતીઇન્સ્યોરન્સ': 3.335323409634415e-06, 'હતાનાયબ': 3.335323409634415e-06, 'હતાગ્રવેગ': 6.67064681926883e-06, 'હતોસંજય': 3.335323409634415e-06, 'હતાજ્': 6.67064681926883e-06, 'હતુંઆ': 1.0005970228903246e-05, 'હતૂં': 6.67064681926883e-06, 'હતાંં': 6.67064681926883e-06, 'હતુંઃ': 1.0005970228903246e-05, 'હતાંસિંહરીંછવાઘ': 3.335323409634415e-06, 'હતીગોલ્ડ': 3.335323409634415e-06, 'હતુંં': 1.0005970228903246e-05, 'હતામાનસ': 3.335323409634415e-06, 'હતીકિંગ્સ': 3.335323409634415e-06, 'હતુંઉલ્લેખનીય': 3.335323409634415e-06, 'હતીઈલાજ': 3.335323409634415e-06, 'હતીઅપહરણ': 3.335323409634415e-06, 'હતીકપિલ': 3.335323409634415e-06, 'હતોપાંચ': 3.335323409634415e-06, 'હતાહાચતુરભાઈ': 3.335323409634415e-06, 'હતાંસંજય': 6.67064681926883e-06, 'હતાકારણ': 6.67064681926883e-06, 'હતાપોલીસ': 6.67064681926883e-06, 'હતા1.': 3.335323409634415e-06, 'હતુંઅગાઉ': 3.335323409634415e-06, 'હતુઆ': 3.335323409634415e-06, 'હતધૈર્ય': 3.335323409634415e-06, 'હતાક્રિસ': 3.335323409634415e-06, 'હતુંવર્લ્ડ': 3.335323409634415e-06, 'હતોય': 3.335323409634415e-06, 'હતીત્રણ': 3.335323409634415e-06, 'હતાંપોલીસ': 3.335323409634415e-06, 'હતાં.': 1.0005970228903246e-05, 'હતોભૂગર્ભ': 3.335323409634415e-06, 'હતોવેલસેકસલાઈફ્': 3.335323409634415e-06, 'હતોપાઠ્ય': 3.335323409634415e-06, 'હતીસાંસદ': 3.335323409634415e-06, 'હતંુનોંધનીય': 3.335323409634415e-06, 'હતાઉલ્લેખનીય': 3.335323409634415e-06, 'હતાંવર્ષ': 3.335323409634415e-06, 'હતીકોંગ્રેસ': 3.335323409634415e-06, 'હતોઆશાભાઈ': 3.335323409634415e-06, 'હતોભારતીય': 3.335323409634415e-06, 'હતીઅપક્ષ': 3.335323409634415e-06, 'હતીસ': 3.335323409634415e-06, 'હતીતિરૂપતિ': 3.335323409634415e-06, 'હતાદિવસ': 3.335323409634415e-06, 'હતુંપણ': 6.67064681926883e-06, 'હતોપોલીસ': 3.335323409634415e-06, 'હતુપોલીસ': 3.335323409634415e-06, 'હતોબીઆરટીએસ': 3.335323409634415e-06, 'હતાંપણ': 3.335323409634415e-06, 'હતાપાટણ': 3.335323409634415e-06, 'હતીપકડાયેલ': 3.335323409634415e-06, 'હતીબીઆરટીએસ': 3.335323409634415e-06, 'હતીમુંબઈ': 3.335323409634415e-06, 'હતીકોલેજ': 3.335323409634415e-06, 'હતીછ': 3.335323409634415e-06, 'હતોમનેય': 3.335323409634415e-06, 'હતોજયોતિરાદિત્ય': 3.335323409634415e-06, 'હતુંબોટાદ': 3.335323409634415e-06, 'હતીફાઇનલ': 3.335323409634415e-06, 'હતીઉલ્લેખનીય': 3.335323409634415e-06, 'હતીતપાસ': 3.335323409634415e-06, 'હતીસમાજ': 3.335323409634415e-06, 'હતીમળેલ': 3.335323409634415e-06, 'હતારવીન્દ્રભાઇ': 3.335323409634415e-06, 'હતાને': 3.335323409634415e-06, 'હતાપીજ': 3.335323409634415e-06, 'હતીમ્યુનિ': 3.335323409634415e-06, 'હતોએચ': 3.335323409634415e-06, 'હતોયાદ': 3.335323409634415e-06}\n",
            "સૌરાષ્ટ્ર {'સૌરાષ્ટ્રમાં': 0.27488151658767773, 'સૌરાષ્ટ્રની': 0.11469194312796209, 'સૌરાષ્ટ્રના': 0.21516587677725119, 'સૌરાષ્ટ્ર': 0.25355450236966826, 'સૌરાષ્ટ્રને': 0.016587677725118485, 'સૌરાષ્ટ્રનું': 0.034123222748815164, 'સૌરાષ્ટ્રીઓ': 0.009478672985781991, 'સૌરાષ્ટ્રનો': 0.035545023696682464, 'સૌરાષ્ટ્રમાંથી': 0.006635071090047393, 'સૌરાષ્ટ્રે': 0.0037914691943127963, 'સૌરાષ્ટ્રનાં': 0.013270142180094787, 'સૌરાષ્ટ્રીઓનો': 0.0009478672985781991, 'સૌરાષ્ટ્રથી': 0.00047393364928909954, 'સૌરાષ્ટ્રી': 0.004739336492890996, 'સૌરાષ્ટ્રએ': 0.004739336492890996, 'સૌરાષ્ટ્રમાંના': 0.0009478672985781991, 'સૌરાષ્ટ્રમાંની': 0.0009478672985781991, 'સૌરાષ્ટ્રમાંનો': 0.0018957345971563982, 'સૌરાષ્ટ્રિક.': 0.00047393364928909954, 'સૌરાષ્ટ્રક': 0.0009478672985781991, 'સૌરાષ્ટ્રીઓની': 0.0009478672985781991, 'સૌરાષ્ટ્ર.': 0.00047393364928909954, 'સૌરાષ્ટ્રનું.': 0.00047393364928909954, 'સૌરાષ્ટ્રીય': 0.0014218009478672985, 'સૌરાષ્ટ્રિય': 0.0018957345971563982, 'સૌરાષ્ટ્રમાંનું': 0.0009478672985781991}\n",
            "મગફળી {'મગફળી': 0.44591439688715956, 'મગફળીની': 0.17509727626459143, 'મગફળીમાં': 0.04202334630350195, 'મગફળીનું': 0.11595330739299611, 'મગફળીના': 0.12684824902723735, 'મગફળીનો': 0.054474708171206226, 'મગફળીનાં': 0.02178988326848249, 'મગફળીને': 0.007003891050583658, 'મગફળીમાંથી': 0.004669260700389105, 'મગફળીઓ': 0.0038910505836575876, 'મગફળીથી': 0.0007782101167315176, 'મગફળી.': 0.0007782101167315176, 'મગફળીનં': 0.0007782101167315176}\n",
            "વાવેતર {'વાવેતર': 0.8538631346578367, 'વાવેતરમાં': 0.05695364238410596, 'વાવેતરનાં': 0.003532008830022075, 'વાવેતરથી': 0.0030905077262693157, 'વાવેતરને': 0.019867549668874173, 'વાવેતરનો': 0.017660044150110375, 'વાવેતરનું': 0.003532008830022075, 'વાવેતરો': 0.003973509933774834, 'વાવેતરના': 0.010154525386313467, 'વાવેતરખરીફ': 0.0008830022075055188, 'વાવેતરની': 0.019867549668874173, 'વાવેતરવાળી': 0.0013245033112582781, 'વાવેતરમાંથી': 0.002207505518763797, 'વાવેતરોમાં': 0.0004415011037527594, 'વાવેતર.': 0.0004415011037527594, 'વાવેતરોમાંથી': 0.0004415011037527594, 'વાવેતરોથી': 0.0008830022075055188, 'વાવેતરથાય': 0.0004415011037527594, 'વાવેતરવાળું': 0.0004415011037527594}\n",
            "રાજકોટ {'રાજકોટ': 0.512893304944405, 'રાજકોટના': 0.12538443340430566, 'રાજકોટની': 0.05488526141471493, 'રાજકોટમાં': 0.19422758457534894, 'રાજકોટમાંથી': 0.004494913650343033, 'રાજકોટઆદ્યકવિ': 0.0002365744026496333, 'રાજકોટરાજકોટ': 0.01726993139342323, 'રાજકોટનું': 0.005441211260941566, 'રાજકોટનાં': 0.0035486160397444995, 'રાજકોટનો': 0.0191625266146203, 'રાજકોટઃ': 0.0011828720132481666, 'રાજકોટસ્વયં': 0.009226401703335699, 'રાજકોટથી': 0.01726993139342323, 'રાજકોટને': 0.01656020818547433, 'રાજકોટરાજ્ય': 0.0009462976105985332, 'રાજકોટભારતીય': 0.0009462976105985332, 'રાજકોટવરાજકોટ': 0.0002365744026496333, 'રાજકોટીન્સ': 0.0002365744026496333, 'રાજકોટઅમદાવાદ': 0.0004731488052992666, 'રાજકોટકોઇ': 0.0002365744026496333, 'રાજકોટએકતરફ': 0.0002365744026496333, 'રાજકોટકચ્છ': 0.0002365744026496333, 'રાજકોટજૂનાગઢ': 0.0004731488052992666, 'રાજકોટીયન્સ': 0.0002365744026496333, 'રાજકોટબોગસ': 0.0007097232079489, 'રાજકોટની.': 0.0002365744026496333, 'રાજકોટખુદ': 0.0002365744026496333, 'રાજકોટનેટ': 0.0002365744026496333, 'રાજકોટમૂળ': 0.0002365744026496333, 'રાજકોટમહાસુદ': 0.0002365744026496333, 'રાજકોટરાહુલ': 0.0002365744026496333, 'રાજકોટકેન્દ્રીય': 0.0004731488052992666, 'રાજકોટઆ': 0.0007097232079489, 'રાજકોટવેલેન્ટાઇન્સ': 0.0004731488052992666, 'રાજકોટજીવલેણ': 0.0002365744026496333, 'રાજકોટના.': 0.0002365744026496333, 'રાજકોટગોંડલ': 0.0002365744026496333, 'રાજકોટવાલીએ': 0.0002365744026496333, 'રાજકોટધુમ્મસ': 0.0002365744026496333, 'રાજકોટન': 0.0002365744026496333, 'રાજકોટસામાન્ય': 0.0002365744026496333, 'રાજકોટવાળા': 0.0002365744026496333, 'રાજકોટઅગાઉ': 0.0002365744026496333, 'રાજકોટસિમેન્ટ': 0.0002365744026496333, 'રાજકોટઆંતરરાષ્ટ્રિય': 0.0002365744026496333, 'રાજકોટરાષ્ટ્રીય': 0.0004731488052992666, 'રાજકોટે': 0.0007097232079489, 'રાજકોટવન્ય': 0.0002365744026496333, 'રાજકોટકહેવાય': 0.0002365744026496333, 'રાજકોટત્રણ': 0.0002365744026496333, 'રાજકોટરાજય': 0.0002365744026496333, 'રાજકોટદક્ષિણ': 0.0002365744026496333, 'રાજકોટએરપોર્ટ': 0.0002365744026496333, 'રાજકોટમુળ': 0.0002365744026496333, 'રાજકોટવર્ષ': 0.0002365744026496333, 'રાજકોટગ્રાન્ટેડ': 0.0002365744026496333, 'રાજકોટધોરણ': 0.0004731488052992666, 'રાજકોટતા': 0.0004731488052992666, 'રાજકોટશ્રાવણ': 0.0002365744026496333, 'રાજકોટઆટકોટ': 0.0002365744026496333, 'રાજકોટઆરોગ્ય': 0.0002365744026496333, 'રાજકોટગરીબ': 0.0002365744026496333, 'રાજકોટઋષભ': 0.0002365744026496333, 'રાજકોટલેબ': 0.0002365744026496333, 'રાજકોટપાડાસણ': 0.0002365744026496333, 'રાજકોટલોકપ્રિય': 0.0002365744026496333, 'રાજકોટસાહિત્ય': 0.0002365744026496333, 'રાજકોટિ': 0.0004731488052992666, 'રાજકોટવાડ': 0.0002365744026496333, 'રાજકોટ.': 0.0002365744026496333, 'રાજકોટજસદણ': 0.0002365744026496333}\n",
            "જામનગર {'જામનગર': 0.43173723783142065, 'જામનગરની': 0.07914523149980214, 'જામનગરમાં': 0.1519588444796201, 'જામનગરના': 0.1590819153146023, 'જામનગરી': 0.06727344677483181, 'જામનગરથી': 0.0664819944598338, 'જામનગરએ': 0.014641867827463396, 'જામનગરસામાન્ય': 0.0003957261574990107, 'જામનગરનું': 0.006331618519984171, 'જામનગરનો': 0.003957261574990107, 'જામનગરરાજયપાલ': 0.0003957261574990107, 'જામનગરસોળ': 0.0003957261574990107, 'જામનગરકાલાવડ': 0.0015829046299960427, 'જામનગરમાંથી': 0.004352987732489117, 'જામનગરને': 0.004352987732489117, 'જામનગરટોલ': 0.0003957261574990107, 'જામનગરઃ': 0.001187178472497032, 'જામનગરભાણવડ': 0.0003957261574990107, 'જામનગરમાં.': 0.0003957261574990107, 'જામનગર.': 0.0003957261574990107, 'જામનગરનું.': 0.0003957261574990107, 'જામનગરધ્રોલ': 0.0007914523149980214, 'જામનગરમાંથી.': 0.0003957261574990107, 'જામનગરજસદણ': 0.0003957261574990107, 'જામનગરશ્રાવણ': 0.0003957261574990107, 'જામનગરનેધરલેન્ડ': 0.0003957261574990107, 'જામનગરનાં': 0.0003957261574990107, 'જામનગરકોંગ્રેસ': 0.0003957261574990107, 'જામનગરે': 0.0003957261574990107, 'જામનગરની.': 0.0003957261574990107, 'જામનગરમુળ': 0.0003957261574990107, 'જામનગરસ્થિત.': 0.0003957261574990107}\n",
            "પોરબંદર {'પોરબંદર': 0.5683453237410072, 'પોરબંદરમાં': 0.13848920863309352, 'પોરબંદરથી': 0.048561151079136694, 'પોરબંદરના': 0.16007194244604317, 'પોરબંદરી': 0.00539568345323741, 'પોરબંદરનું': 0.017985611510791366, 'પોરબંદર.': 0.0017985611510791368, 'પોરબંદરનો': 0.01079136690647482, 'પોરબંદરની': 0.04136690647482014, 'પોરબંદરનાં': 0.0017985611510791368, 'પોરબંદરઃ': 0.0017985611510791368, 'પોરબંદરને': 0.0035971223021582736}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2C-Nr-GMdGv",
        "colab_type": "text"
      },
      "source": [
        "# Word2Vec Model Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci3cGQYmWk63",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "> get data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRBUvX8WgNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gujarati_line_with_quote(line):\n",
        "\n",
        "    index = 0\n",
        "    le = 0\n",
        "    quotes = 0\n",
        "    final_list = list()\n",
        "    temp = str()\n",
        "    for i in range(len(line)):\n",
        "        le += 1\n",
        "        if '\"' == line[i]:\n",
        "            quotes += 1\n",
        "            if quotes == 2:\n",
        "                temp = line[index:i].strip()\n",
        "\n",
        "                if temp is not '':\n",
        "                    final_list.append(temp + '*')\n",
        "                quotes = 0\n",
        "            else:\n",
        "                temp = line[index:i].split()\n",
        "                for j in temp:\n",
        "                    if temp is not '':\n",
        "                        final_list.append(j)\n",
        "            index = i + 1\n",
        "\n",
        "    if index < le:\n",
        "        temp = line[index:le].split()\n",
        "        for i in temp:\n",
        "            final_list.append(i.strip())\n",
        "    return final_list\n",
        "\n",
        "\n",
        "def stemmingData(sample):\n",
        "    stopWords = StopWords()\n",
        "    final_list = stopWords.sw_remove(sample)\n",
        "\n",
        "    sr = SuffixRemoval()\n",
        "    for sentence_words_index in range(len(final_list)):\n",
        "        for word_index in range(len(final_list[sentence_words_index])):\n",
        "            if final_list[sentence_words_index][word_index].count('*') > 0:\n",
        "                final_list[sentence_words_index][word_index] = final_list[sentence_words_index][word_index].strip('*')\n",
        "                print(final_list[sentence_words_index][word_index])\n",
        "                continue\n",
        "            final_list[sentence_words_index][word_index] = sr.suffixRemoval(\n",
        "                final_list[sentence_words_index][word_index]).get(\n",
        "                'word')\n",
        "\n",
        "    return final_list\n",
        "\n",
        "\n",
        "def pure_gujarati_need_for_data(dataset_file_loc, string_based=0, lines=None):\n",
        "    sample, lines = need_for_data_sample(dataset_file_loc, string_based, lines)\n",
        "    f = sample.replace(\"\\n\", \".\")  # Replaces escape character with space\n",
        "    L = f.split('.')\n",
        "\n",
        "    final_list = list()\n",
        "\n",
        "    for i in range(len(L)):\n",
        "        if '\"' in L[i]:\n",
        "            final_list.append(gujarati_line_with_quote(L[i]))\n",
        "        else:\n",
        "            if L[i] is not \"\":\n",
        "                final_list.append(L[i].split())\n",
        "    print(\"Total sentence Used\", len(final_list))\n",
        "\n",
        "    return stemmingData(final_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNyAgRq4MiWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "\n",
        "def need_for_data_sample(file_loc, string_based, lines=None):\n",
        "    # if it's string based\n",
        "    if string_based is 1:\n",
        "        sample = input(\"Enter String \")\n",
        "\n",
        "    # if it's file based\n",
        "    else:\n",
        "        file = open(file_loc, 'rt', encoding=\"UTF-8\")\n",
        "        # if number of lines not provided\n",
        "        if lines is None:\n",
        "            lines = len(file.readlines())\n",
        "        sample = str()\n",
        "        file.seek(0, 0)\n",
        "        for i in range(lines):\n",
        "            sample += file.readline()\n",
        "\n",
        "    return sample, lines\n",
        "\n",
        "\n",
        "def dataCleaning(file_loc):\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenized_sentences = tokenizer.sentenceTokenizer(file=file_loc)\n",
        "    tokenized_words = tokenizer.wordTokenizer(tokenized_sentences)\n",
        "    total_words = 0\n",
        "    del tokenizer\n",
        "    del tokenized_sentences\n",
        "    stopWords = StopWords()\n",
        "    data = stopWords.sw_remove(tokenized_words)\n",
        "\n",
        "    del tokenized_words\n",
        "\n",
        "    sr = SuffixRemoval()\n",
        "    for sentence_words_index in range(len(data)):\n",
        "        for word_index in range(len(data[sentence_words_index])):\n",
        "            data[sentence_words_index][word_index] = sr.suffixRemoval(data[sentence_words_index][word_index]).get(\n",
        "                'word')\n",
        "            total_words += 1\n",
        "\n",
        "    return data, total_words\n",
        "\n",
        "\n",
        "# DONE\n",
        "def need_for_data(file_loc, string_based, lines=None):\n",
        "    data, total_words = dataCleaning(file_loc)\n",
        "\n",
        "    # find total numbers of line in file\n",
        "    file = open(file_loc, 'rt', encoding=\"UTF-8\")\n",
        "    print(\"\\nTotal No of Lines in FILE :\", (len(file.readlines())))\n",
        "    print(\"Total No of words used for training model :\", total_words)\n",
        "    file.close()\n",
        "    return data\n",
        "\n",
        "\n",
        "# DONE\n",
        "def skip_gram_model(dataset_file_loc, model_name, file_dir_to_save, string_based=0, lines=None, hs=0, pure_gujarati=0):\n",
        "    if pure_gujarati is 1:\n",
        "        data = pure_gujarati_need_for_data(dataset_file_loc=dataset_file_loc, lines=lines, string_based=string_based)\n",
        "    else:\n",
        "        data = need_for_data(dataset_file_loc, lines=lines, string_based=string_based)\n",
        "\n",
        "    print(\"\\nSKIP GRAM\\n\" + \"=\" * 15)\n",
        "    print(\"Training Model using Skip Gram \")\n",
        "    sg_model = gensim.models.Word2Vec(data, min_count=1, size=300, window=5,\n",
        "                                      sg=1, workers=1, hs=hs)  # add worker in argument for multi threading\n",
        "    sg_model.save(file_dir_to_save + '/' + model_name)  # to save the trained model\n",
        "    print(\"\\nTotal Training time : \", sg_model.total_train_time)\n",
        "    print(\"SKIP GRAM MODEL saved successfully...\")\n",
        "    print(\"At location :\" + file_dir_to_save + \"/\" + model_name)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlAbHTMXHQEi",
        "colab_type": "text"
      },
      "source": [
        "# Implementation of tokenizer, suffix removal and stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2rItihdGySp",
        "colab_type": "code",
        "outputId": "161e2366-b78b-44ae-a815-cade2549530f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!7z e \"/content/drive/My Drive/DataSet/cleanedDataSet/combined_cleanedDataSet.7z\"\n",
        "\n",
        "from sys import getsizeof\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "t = Tokenizer()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-9901f7611f64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'7z e \"/content/drive/My Drive/DataSet/cleanedDataSet/combined_cleanedDataSet.7z\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetsizeof\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    436\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m   result = _run_command(\n\u001b[0;32m--> 438\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    439\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     with temporary_clearer(), _display_stdin_widget(\n\u001b[0;32m--> 181\u001b[0;31m         delay_millis=500) as update_stdin_widget:\n\u001b[0m\u001b[1;32m    182\u001b[0m       \u001b[0;31m# TODO(b/115531839): Ensure that subprocesses are terminated upon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0;31m# interrupt.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_display_stdin_widget\u001b[0;34m(delay_millis)\u001b[0m\n\u001b[1;32m    339\u001b[0m   \u001b[0mshell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m   \u001b[0mdisplay_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'cell_display_stdin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'delayMillis'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdelay_millis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m   \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdisplay_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_header\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mecho_updater\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_echo_status\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0;31m# unique.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0mrequest_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msend_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZRnLRhbGNkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a402db07-f40b-4635-d6a3-30b250ac894d"
      },
      "source": [
        "gujStem = GujaratiStemmer()\n",
        "\n",
        "guStemmedData = gujStem.guStemmer(filePath = \"/content/combined_cleanedDataSet.txt\", cleanData= True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fb12e327b38c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgujStem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGujaratiStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mguStemmedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgujStem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguStemmer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilePath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/combined_cleanedDataSet.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcleanData\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-c8784bb2c1a1>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         self.guDict = readJsonFile(\n\u001b[0;32m----> 4\u001b[0;31m             r\"/content/drive/My Drive/Gujarati chatbot/stemmer2/helpingFiles/new_gu_dict_letters_python_dictionary.json\")\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSuffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-a6b2d7157d84>\u001b[0m in \u001b[0;36mreadJsonFile\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mreadJsonFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7eu7qit8FZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in guStemmedData[0:10]:\n",
        "  print(i)\n",
        "\n",
        "total_words = 0\n",
        "\n",
        "for tokenized_sentences in guStemmedData:\n",
        "  total_words += len(tokenized_sentences)\n",
        "\n",
        "print(\"Total words :\", total_words)\n",
        "print(\"Total Execution Time : %s seconds\" %(time.time() - start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWhZj65uM6rB",
        "colab_type": "text"
      },
      "source": [
        "# implementation Training Word2Vec Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTAw2JHgNB3R",
        "colab_type": "code",
        "outputId": "f3b1c835-b940-43ef-fd3e-5401d00d2ae8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        }
      },
      "source": [
        "starting_time = time.time()\n",
        "\n",
        "data = word_tokenized_sentences\n",
        "sg_model = gensim.models.Word2Vec(data, min_count=1, size=300, window=5,\n",
        "                                      sg=1, workers=10)  # add worker in argument for multi threading\n",
        "sg_model.save(\"combined_cleanedDataSet.w2v\")  # to save the trained model\n",
        "\n",
        "#sg_model.save(\"/content/drive/My Drive/combined_cleanedDataSet.w2v\")\n",
        "\n",
        "ending_time = time.time()\n",
        "print(\"Total execution time :\", ending_time-starting_time)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-aaee31a2f273>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstarting_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenized_sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m sg_model = gensim.models.Word2Vec(data, min_count=1, size=300, window=5,\n\u001b[1;32m      5\u001b[0m                                       sg=1, workers=10)  # add worker in argument for multi threading\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_tokenized_sentences' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68Y12b5enB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloadFile(sg_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kuf8mTx5CG-8",
        "colab_type": "text"
      },
      "source": [
        "# create Xgram and Xgram with add One Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqvHN4lC1eWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here problem in addOneSmoothing we are dividing using xMinusOneGramCounts instead\n",
        "# of total len(listOfXminusOneGramCount )\n",
        "# consider and edit it \n",
        "# cause EX: consider \"x\" : 3000 occurunce but len(XminusOneGramSmoothing) is < 3000\n",
        "# then probability will be greater then 1 which is not possible \n",
        "# so modify it\n",
        "def xgramWithAddOneSmoothing(listOfXgrams, xgramCounts, xMinusOneGramCounts, x):\n",
        "    listOfProb = {}\n",
        "    cStar = {}\n",
        "    xMinusOneGramCountsLength = len(xMinusOneGramCounts) - x + 1\n",
        "\n",
        "    for xgram in listOfXgrams:\n",
        "        listOfProb[xgram] = (xgramCounts.get(xgram) + 1) / (xMinusOneGramCounts.get(x[0:x-1]) +\n",
        "                                                            xMinusOneGramCountsLength \n",
        "                                                            )\n",
        "        cStar[xgram] = (xgramCounts.get(xgram) + 1) * xMinusOneGramCounts.get(xgram[0:x-1]) / (\n",
        "            xMinusOneGramCounts.get(xgram[0:-x]) + xMinusOneGramCountsLength\n",
        "        )\n",
        "\n",
        "    return listOfProb, cStar\n",
        "\n",
        "def createXgram(words_data, x):\n",
        "    \"\"\"\n",
        "        words_data must be cleaned and striped with space...\n",
        "    :param words_data: list of words\n",
        "    :param x: maxGram number (like for trigram x=3)\n",
        "    :return: all the grams 1<= gram <= x are returned\n",
        "    \"\"\"\n",
        "    AG = {}  # allGrams\n",
        "    dataLength = len(words_data)\n",
        "\n",
        "    for i in range(1, x + 1):\n",
        "        listOfXgram = 'listOf' + str(i) + 'gram'\n",
        "        xgramCounts = str(i) + 'gramCounts'\n",
        "        AG[i] = {\n",
        "            listOfXgram: [],\n",
        "            xgramCounts: {}\n",
        "        }\n",
        "\n",
        "        print(\"Creating the {}gram...\".format(str(i)))\n",
        "\n",
        "        for wordIndex in range(len(words_data)):\n",
        "            if wordIndex < dataLength - i + 1:\n",
        "                tpl = tuple(words_data[wordIndex: wordIndex + i])\n",
        "                AG[i][listOfXgram].append(tpl)\n",
        "\n",
        "                if tpl in AG[i][xgramCounts]:\n",
        "                    AG[i][xgramCounts][tpl] += 1\n",
        "                else:\n",
        "                    AG[i][xgramCounts][tpl] = 1\n",
        "\n",
        "    return AG\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOMFh3cLDFx7",
        "colab_type": "text"
      },
      "source": [
        "#prepare dataset for creating xgrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng8dgC7-DInv",
        "colab_type": "code",
        "outputId": "9a26e18c-0fd2-421f-b4a3-6573b937d4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "file = open(r\"/content/drive/My Drive/DataSet/combined_cleanedDataSet.txt\", 'rt', encoding='utf-8')\n",
        "sent_data = file.readlines()\n",
        "file.close()\n",
        "words_data = []\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~‘ '\\n'''\n",
        "\n",
        "for i in range(len(sent_data)):\n",
        "  sent_data[i] = sent_data[i].strip(punctuations).split()\n",
        "  words_data.extend(sent_data[i])\n",
        "\n",
        "print(\"total words :\",len(words_data))\n",
        "print(sent_data[:5])\n",
        "print(words_data[:13])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total words : 18118821\n",
            "[['પ્રસ્તુત', 'થતી', 'નવી', 'કૃતિઓની', 'ઝલક', 'મેળવો', 'આપના', 'આપનું', 'ઈ-મેલ', 'એડ્રેસ', 'લખો'], ['અક્ષરનાદ'], ['કોમ', 'વેબસાઈટ', 'ગુજરાતી', 'સાહિત્યને', 'માધ્યમથી', 'વિશ્વના', 'વિવિધ', 'વિભાગોમાં', 'વસતા', 'ગુજરાતીઓ', 'સુધી', 'પહોંચાડવાનો', 'તદ્દન', 'અવ્યાવસાયિક', 'પ્રયાસ', 'છે'], ['અક્ષરનાદ', 'પર', 'થતી', 'નવી', 'કૃતિઓની', 'ઝલક', 'મેળવો', 'આપના', 'ઈનબોક્સમાં', 'આપનું', 'ઈ-મેલ', 'એડ્રેસ', 'લખો'], ['કોમ', 'વેબસાઈટ', 'ગુજરાતી', 'સાહિત્યને', 'ઈન્ટરનેટના', 'માધ્યમથી', 'વિશ્વના', 'વિવિધ', 'વિભાગોમાં', 'વસતા', 'ગુજરાતીઓ', 'સુધી', 'પહોંચાડવાનો', 'તદ્દન', 'અવ્યાવસાયિક', 'પ્રયાસ', 'છે']]\n",
            "['પ્રસ્તુત', 'થતી', 'નવી', 'કૃતિઓની', 'ઝલક', 'મેળવો', 'આપના', 'આપનું', 'ઈ-મેલ', 'એડ્રેસ', 'લખો', 'અક્ષરનાદ', 'કોમ']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_7DRDGGDNJ6",
        "colab_type": "text"
      },
      "source": [
        "#Execute Create Xgram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2IS_KNUXs3W",
        "colab_type": "text"
      },
      "source": [
        "Prepare Words Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwGmIY2XDM1i",
        "colab_type": "code",
        "outputId": "8714bf96-18d8-4e39-cdf6-f6fe4033caaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "import time\n",
        "startTime = time.time()\n",
        "dataCleaner = DataCleaner()\n",
        "\n",
        "file = open(\"/content/combined_cleanedDataSet.txt\", 'rt', encoding='utf8')\n",
        "data = file.read()\n",
        "file.close()\n",
        "\n",
        "wordsData = dataCleaner.cleanData( data=data,removeRedundancy=False, replaceNewLineUsing=' ', splitSentencesUsing=None, joinSentencesUsing=' ')\n",
        "wordsData = wordsData.split()\n",
        "\n",
        "gujStem = GujaratiStemmer()\n",
        "\n",
        "for wordIndex in range(len(wordsData)):\n",
        "    wordsData[wordIndex] = gujStem.guWordStemmer(wordsData[wordIndex].strip())\n",
        "\n",
        "print(\"First 25 words\\n\",\"-\"*40)\n",
        "for w in wordsData[0:10]:\n",
        "    print(w)\n",
        "\n",
        "print(\"Total words :\", len(wordsData))\n",
        "print(\"Total time :\", time.time() - startTime)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total text length : 174318574\n",
            "Total sentences : 1\n",
            "0\n",
            "First 25 words\n",
            " ----------------------------------------\n",
            "આ\n",
            "સવ્યભ્રમણ\n",
            "ઉત્તરધ્રુવ\n",
            "દેખ\n",
            "છે\n",
            "સરસ્વતીચંદ્ર\n",
            "ઝડપ\n",
            "દોડ\n",
            "જવ\n",
            "હડી\n",
            "Total words : 28633782\n",
            "Total time : 184.62774848937988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OWoJXRtX6nK",
        "colab_type": "text"
      },
      "source": [
        "Get Xgram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA848kNoYCXv",
        "colab_type": "code",
        "outputId": "00e07ed0-738c-4c9e-a56d-a7bf7988d589",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "startTime = time.time()\n",
        "xGrams = createXgram(words_data=wordsData, x=4)\n",
        "print(\"Time taken by XGrams Generation :\", time.time()-startTime)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the 1gram...\n",
            "Creating the 2gram...\n",
            "Creating the 3gram...\n",
            "Creating the 4gram...\n",
            "Time taken by XGrams Generation : 157.00533819198608\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtcxcXX9DRQ1",
        "colab_type": "text"
      },
      "source": [
        "#all grams probability and calculate probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCv2VJypDQ9K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calAllGramsProb(xGrams):\n",
        "  if len(xGrams.keys()) <= 1:\n",
        "      return\n",
        "  else:\n",
        "      allXgramsProb = {}\n",
        "\n",
        "      allXgrams = list(xGrams.keys())\n",
        "      allXgrams.reverse()\n",
        "\n",
        "      for key in xGrams:\n",
        "          listOfXgram = 'listOf' + str(key) + 'gram'\n",
        "          xgramCounts = str(key) + 'gramCounts'\n",
        "          xMinusOneGramCounts = str(key-1) + 'gramCounts'\n",
        "          \n",
        "          if key == 1:\n",
        "              allXgramsProb[key] = calcXgramProb(xGrams[key].get(listOfXgram), xgramCounts=xGrams[key].get(xgramCounts), \n",
        "                    xMinusOneGramCounts=None, x=key )\n",
        "              continue\n",
        "\n",
        "          allXgramsProb[key] = calcXgramProb(xGrams[key].get(listOfXgram), xgramCounts=xGrams[key].get(xgramCounts), \n",
        "                              xMinusOneGramCounts=xGrams[key-1].get(xMinusOneGramCounts), x=key )\n",
        "          \n",
        "      return allXgramsProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnpgbd90DXpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calcXgramProb(listOfXgram, xgramCounts, xMinusOneGramCounts, x):\n",
        "    listOfProb = {}\n",
        "\n",
        "    if x == 1:\n",
        "        totalTuple = len(listOfXgram)\n",
        "        for xgram in listOfXgram:\n",
        "            listOfProb[xgram] = (xgramCounts.get(xgram)) / (totalTuple)\n",
        "   \n",
        "        return listOfProb\n",
        "   \n",
        "    for xgram in listOfXgram:\n",
        "        listOfProb[xgram] = (xgramCounts.get(xgram)) / (xMinusOneGramCounts.get(xgram[0:x - 1]))\n",
        "\n",
        "    return listOfProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9yq0hnHDYZB",
        "colab_type": "text"
      },
      "source": [
        "#calculate probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4sCi351DhQa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "559eac3b-c199-466e-e2b0-59ed7121f75f"
      },
      "source": [
        "allXgramsProb = calAllGramsProb(xGrams)\n",
        "\n",
        "for i in list(allXgramsProb[1].keys())[0:10]:\n",
        "    print(i,allXgramsProb[1].get(i))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('આ',) 0.011338145970378626\n",
            "('સવ્યભ્રમણ',) 6.98475667657175e-08\n",
            "('ઉત્તરધ્રુવ',) 7.333994510400338e-07\n",
            "('દેખ',) 0.0003552796483538221\n",
            "('છે',) 0.03864005809641213\n",
            "('સરસ્વતીચંદ્ર',) 8.835717195863264e-06\n",
            "('ઝડપ',) 0.0002499844414545029\n",
            "('દોડ',) 0.00018565483246327713\n",
            "('જવ',) 0.0008488924026871477\n",
            "('હડી',) 2.7589788872458415e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B44Uvxd4DxCR",
        "colab_type": "text"
      },
      "source": [
        "#highest Probability Sentence Creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hxakjB-Dwj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def highestProbabilitySentenceCreator(text, XgramProb, x):\n",
        "    text = text.split()\n",
        "\n",
        "    if len(text) < x:\n",
        "        # text is smaller than the x then replace it with x + 1\n",
        "        x = len(text) + 1\n",
        "\n",
        "\n",
        "    if x > max(list(XgramProb.keys())):\n",
        "        # if x is higher than highest available XgramProb \n",
        "        # than replace it with  highest available XgramProb \n",
        "        x = max(list(XgramProb.keys()))\n",
        "\n",
        "    sentence_finished = False\n",
        "\n",
        "    while not sentence_finished:\n",
        "\n",
        "        highestProb = 0\n",
        "        hpTuple = ()\n",
        "        \n",
        "        for tpl in XgramProb[x].keys():\n",
        "            flag = True\n",
        "            for t in range(x - 1):\n",
        "                if tpl[t] != text[ - x + 1 + t]:\n",
        "                    flag = False\n",
        "            if flag is True:\n",
        "                if highestProb <= XgramProb[x][tpl]:\n",
        "                    highestProb = XgramProb[x][tpl]\n",
        "                    hpTuple = tpl\n",
        "\n",
        "        noneFlag = 0\n",
        "        for t in range(x):\n",
        "            if len(text) < x:\n",
        "                break\n",
        "            if text[-x + t] is None:\n",
        "                noneFlag += 1\n",
        "\n",
        "        if noneFlag == x:\n",
        "            sentence_finished = True\n",
        "\n",
        "        if len(text) > 8:\n",
        "            sentence_finished = True\n",
        "\n",
        "        if len(hpTuple) == 0:\n",
        "            text.append(None)\n",
        "            continue\n",
        "        \n",
        "        text.append(hpTuple[len(hpTuple) - 1])\n",
        "        \n",
        "\n",
        "\n",
        "    return ' '.join([t for t in text if t])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ZeEy8ZD866",
        "colab_type": "text"
      },
      "source": [
        "# get Top N items from dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB0wZ5T3D9yq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTopNItems(dictionary, n):\n",
        "    tempDictionary = {k:v for k,v in sorted(dictionary.items(), key= lambda item : item[1], reverse=True) }\n",
        "    result = {}\n",
        "    count = 0     \n",
        "  \n",
        "    for k,v in tempDictionary.items():\n",
        "        if count == n:\n",
        "            break\n",
        "        else:\n",
        "            count += 1\n",
        "            result[k] = v\n",
        "    return result\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkFUBtGzD_4H",
        "colab_type": "text"
      },
      "source": [
        "#top N Possibal Sentences creator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NujGOiAUEDIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTopNPossibalSentences(text, xgramProb, x, n=1, maxSentenceLength=5):\n",
        "    text = tuple(text.split())\n",
        "\n",
        "    if len(text) < x:\n",
        "        # text is smaller than the x then replace it with x + 1\n",
        "        x = len(text) + 1\n",
        "\n",
        "    if x > max(list(xgramProb.keys())):\n",
        "        # if x is higher than highest available XgramProb \n",
        "        # than replace it with  highest available XgramProb \n",
        "        x = max(list(xgramProb.keys()))\n",
        "      \n",
        "    done = False\n",
        "    # initializing the dictnionary for start\n",
        "    results = {\n",
        "        text: None\n",
        "    }\n",
        "\n",
        "    iterations = maxSentenceLength - len(text)\n",
        "    print(\"Have to find next\", iterations, \"words\")\n",
        "\n",
        "    while iterations != 0:\n",
        "        intermediateResult = {}\n",
        "\n",
        "        for tpl in results.keys():\n",
        "            intermediateResult.update(nextNPossibalWords(inpTpl=tpl, \n",
        "                                                         xgramProb=xgramProb, \n",
        "                                                         x=x, n=n))\n",
        "\n",
        "        results = getTopNItems(dictionary=intermediateResult,n=n)\n",
        "        iterations -= 1\n",
        "\n",
        "    finalResult = {}    \n",
        "    for k,v in results.items():\n",
        "        finalResult[' '.join(w for w in k if k)] = v\n",
        "    \n",
        "    return finalResult"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-CQpwH3EKPb",
        "colab_type": "text"
      },
      "source": [
        "# RUN top N sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKGIIve_EGiK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getTopNPossibalSentences(text='ગુજરાત નરેન્દ્ર', xgramProb=allXgramsProb, x=3, n=3, \n",
        "                        maxSentenceLength=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MRrZ7VLEpG-",
        "colab_type": "text"
      },
      "source": [
        "# Sentence probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1EP2_TnEqkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getTupleProb(tpl, xgramsProb, x):\n",
        "    if len(tpl) > 0:\n",
        "        if tpl not in xgramsProb[x]:\n",
        "            if x == 1:\n",
        "                xgramsProb[x][tpl] = (1) / (1+ len(xgramsProb[x]))\n",
        "            else:\n",
        "                return getTupleProb(tpl[0:x-1], xgramsProb, x-1 ) * getTupleProb(tpl[x-1:], xgramsProb, 1 )\n",
        "\n",
        "        return xgramsProb[x][tpl]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZOhgCNDEsQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_xgram_sentence_probability(sentence,xgramsProb, x=2):\n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~‘'''\n",
        "    outputProb = 1\n",
        "    tplList = []\n",
        "    words_tokenized = sentence.split()\n",
        "\n",
        "    if x not in xgramsProb:\n",
        "        x = max(list(xgramsProb.keys()))\n",
        "    if len(words_tokenized) < x :\n",
        "        x = len(words_tokenized)\n",
        "\n",
        "    \n",
        "\n",
        "    for wordIndex in range(len(words_tokenized)):\n",
        "        words_tokenized[wordIndex] = words_tokenized[wordIndex].strip(punctuations)\n",
        "\n",
        "    print(\"INPUT :\", ' '.join([word for word in words_tokenized if word]))\n",
        "\n",
        "    for wordIndex in range(len(words_tokenized) - x + 1):\n",
        "        temp = []\n",
        "        for tplWordIndex in range(wordIndex, wordIndex + x):\n",
        "            temp.append(words_tokenized[tplWordIndex])\n",
        "        tplList.append(tuple(temp.copy()))\n",
        "\n",
        "    for i in tplList:\n",
        "        outputProb *= getTupleProb(tpl=i, xgramsProb=xgramsProb, x=x)\n",
        "    return outputProb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vX2EcFIEN-C",
        "colab_type": "text"
      },
      "source": [
        "#get Next N possibal words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kftF4I-jESmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def nextNPossibalWords(inpTpl, xgramProb, x, n):\n",
        "    inpTplLength = len(inpTpl)\n",
        "\n",
        "    if inpTplLength < x - 1:\n",
        "        # text is smaller than the x then replace it with len(inpTpl) + 1\n",
        "        x = inpTplLength + 1\n",
        "\n",
        "    if x > max(list(xgramProb.keys())):\n",
        "        # if x is higher than highest available XgramProb \n",
        "        # than replace it with  highest available XgramProb \n",
        "        x = max(list(xgramProb.keys()))\n",
        "  \n",
        "    # to be solved cause if tpl > max(xgram) then what?\n",
        "\n",
        "    possibalWords = {}\n",
        "\n",
        "    # if x not found in xgramProb ex: for x=0\n",
        "    if xgramProb.get(x) == None:\n",
        "        return possibalWords\n",
        "\n",
        "    for tpl in xgramProb[x].keys():\n",
        "            flag = True \n",
        "            for t in range(x - 1):\n",
        "                if tpl[t] != inpTpl[inpTplLength + t - x + 1]:\n",
        "                    flag = False\n",
        "                    break\n",
        "            if flag is True:\n",
        "                temp = list(inpTpl)\n",
        "                temp.append(tpl[-1])\n",
        "                temp = tuple(temp)\n",
        "                possibalWords[temp] = xgramProb[x][tpl]\n",
        "    # means no match found\n",
        "    if list(possibalWords.keys()) == []:\n",
        "        if x != 1:\n",
        "            return nextNPossibalWords(inpTpl=inpTpl, xgramProb=xgramProb, \n",
        "                                      x=x-1, n=n)\n",
        "        elif x == 1:\n",
        "            return possibalWords\n",
        "    \n",
        "    return getTopNItems(possibalWords, n=n)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osoVryqbEhPe",
        "colab_type": "text"
      },
      "source": [
        "# RUN Get Next N possible words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UeaD69oEVa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nextNPossibalWords(('ગુજરાત', 'નરેન્દ્ર'),allXgramsProb, 3, 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBjgauEcE1Lp",
        "colab_type": "text"
      },
      "source": [
        "# calculate sentence Probability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paMx9-rtE0oc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1HDix4GDmwo",
        "colab_type": "text"
      },
      "source": [
        "#Pickle All data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_DdCbhUl0pa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b3cd611b-b4a4-4e2f-b62c-b86f98c72dd9"
      },
      "source": [
        "import sys\n",
        "\n",
        "print(\"sent_data :\",sys.getsizeof(sent_data)/1000, \" Mb\")\n",
        "print(\"Words_data :\",sys.getsizeof(words_data)/1000, \" Mb\")\n",
        "print(\"data :\",sys.getsizeof(data)/1000, \" Mb\")\n",
        "print(\"wordsData :\",sys.getsizeof(wordsData)/1000, \" Mb\")\n",
        "print(\"xGrams :\",sys.getsizeof(xGrams)/1000, \" Mb\")\n",
        "print(\"allXgramsProb :\", sys.getsizeof(allXgramsProb)/100, \"Mb\")\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sent_data : 9281.488  Mb\n",
            "Words_data : 151598.72  Mb\n",
            "data : 348637.222  Mb\n",
            "wordsData : 251133.496  Mb\n",
            "xGrams : 0.24  Mb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d376827e8756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wordsData :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordsData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Mb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xGrams :\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxGrams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" Mb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"allXgramsProb :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsizeof\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallXgramsProb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Mb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'allXgramsProb' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTVulXRftNP3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "7fef4891-9f5d-41a6-ed7f-c975d57aacb9"
      },
      "source": [
        "del sent_data\n",
        "del words_data\n",
        "del data\n",
        "del wordsData"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-075e953d0590>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0msent_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mwords_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mwordsData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'sent_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InDOZ8M6DmMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "file = open(r\"/content/drive/My Drive/Output/xgrams.pickle\", 'wb')\n",
        "pickle.dump(xGrams, file)\n",
        "file.close()\n",
        "del xGrams"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC5rIA8Etj5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = open(r\"/content/drive/My Drive/Output/xgramsProb.pickle\", 'wb')\n",
        "pickle.dump(allXgramsProb, file)\n",
        "file.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}