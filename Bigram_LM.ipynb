{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bigram_LM.ipynb","provenance":[],"collapsed_sections":["1ll7utCF6QRX","WnoVrDtQKSBX","Wvpm9Wnv-auQ","3xRmdeXSE3eX","2PfddoybIYFa"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"qDVXWWQDyfHu","colab_type":"text"},"source":["### Import required libraries including NLTK and Reuters Dataset"]},{"cell_type":"code","metadata":{"id":"nO887GAc2IlP","colab_type":"code","outputId":"356f2141-038e-45e6-9d67-35a6d4ee2cdf","executionInfo":{"status":"ok","timestamp":1583669995435,"user_tz":-330,"elapsed":3794,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}},"colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["\n","import nltk\n","nltk.download('reuters') # one time execution\n","nltk.download('punkt')  # one time execution\n","\n","from nltk.corpus import reuters\n","import math\n","import random\n","#Done"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package reuters to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IWGjl_py5yP5","colab_type":"text"},"source":["### Know the data:"]},{"cell_type":"code","metadata":{"id":"gPukN5JHKVKz","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tnS_OL2bEvw","colab_type":"code","outputId":"a5f000cb-b440-4c16-f540-87b89cd63979","executionInfo":{"status":"ok","timestamp":1583670023705,"user_tz":-330,"elapsed":1903,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}},"colab":{"base_uri":"https://localhost:8080/","height":55}},"source":["# see the data\n","reuters.raw()[:200]"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reachin\""]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"1T4e_ZpBy00z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":641},"outputId":"e359d6ad-8553-4c53-8edc-8d1e728a9ffb","executionInfo":{"status":"ok","timestamp":1583670044648,"user_tz":-330,"elapsed":1344,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# get sentences\n","dataset_sents = reuters.sents()\n","# print first sentence\n","dataset_sents[1]\n","#Done"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['They',\n"," 'told',\n"," 'Reuter',\n"," 'correspondents',\n"," 'in',\n"," 'Asian',\n"," 'capitals',\n"," 'a',\n"," 'U',\n"," '.',\n"," 'S',\n"," '.',\n"," 'Move',\n"," 'against',\n"," 'Japan',\n"," 'might',\n"," 'boost',\n"," 'protectionist',\n"," 'sentiment',\n"," 'in',\n"," 'the',\n"," 'U',\n"," '.',\n"," 'S',\n"," '.',\n"," 'And',\n"," 'lead',\n"," 'to',\n"," 'curbs',\n"," 'on',\n"," 'American',\n"," 'imports',\n"," 'of',\n"," 'their',\n"," 'products',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"Q63sKCJxy9gc","colab_type":"code","outputId":"c52f214e-1757-4531-a601-9c9957b40ec5","executionInfo":{"status":"ok","timestamp":1583670107458,"user_tz":-330,"elapsed":7223,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# number of sentences\n","len(dataset_sents)"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["54716"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"6SgWoXcXzIcF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f26a5aa1-bfc0-4fbc-a546-bcdb4e300f01","executionInfo":{"status":"ok","timestamp":1583670113845,"user_tz":-330,"elapsed":1514,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# get list of words\n","dataset_words = reuters.words()\n","# print first word\n","dataset_words[:10]"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN']"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"24IEtnUAzeCc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"8ccece55-8d9c-41ce-f9b0-f828a87a9d3b","executionInfo":{"status":"ok","timestamp":1583670124047,"user_tz":-330,"elapsed":4644,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# total number of words\n","len(dataset_words)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1720901"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"00Do0vfczlOb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"acdaf41e-54f7-4abd-c300-52b44691f615","executionInfo":{"status":"ok","timestamp":1583670130247,"user_tz":-330,"elapsed":3817,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# size of vocabulary\n","len(set(dataset_words))"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["41600"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"UnXiiFQ759qH","colab_type":"text"},"source":["### Split the Dataset:"]},{"cell_type":"markdown","metadata":{"id":"reMY08314XMK","colab_type":"text"},"source":["Divide the dataset into two Train data and Test data. Bigram Probabilities are learnt on the Train data and to evaluate it, we use the Test data. <br>\n","Out of 54716 sentences, 40000 sentences are used for training."]},{"cell_type":"code","metadata":{"id":"KUiQqArqkwEQ","colab_type":"code","colab":{}},"source":["data_sents = dataset_sents[:40000]\n","data_sents_test = dataset_sents[40000:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g65ozpPV5JSC","colab_type":"text"},"source":["Get lists of tokens of train and test data."]},{"cell_type":"code","metadata":{"id":"VxKG3jG_0dsD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"7bb07f13-2090-4d8f-ce4e-df33c81f49cf","executionInfo":{"status":"ok","timestamp":1583670226243,"user_tz":-330,"elapsed":5778,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# number of words in train data\n","num_words = 0\n","for sentence in data_sents:\n","  num_words += len(sentence)\n","num_words"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1262448"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"xhoLQv-LUEtU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O0QYIpaIjlmu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"98cc1acb-dafa-46c1-b1ef-98c85e5cfda4","executionInfo":{"status":"ok","timestamp":1583670234243,"user_tz":-330,"elapsed":1444,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["num_words"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1262448"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"l-rz7ZIO0iAN","colab_type":"code","colab":{}},"source":["# create two lists containing words\n","data_words_train = dataset_words[:num_words]\n","data_words_test = dataset_words[num_words:]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VcSunRes6I8e","colab_type":"text"},"source":["### Learn Probabilities"]},{"cell_type":"markdown","metadata":{"id":"cXbRsLiH6sk-","colab_type":"text"},"source":["Method to generate a list of bigrams, and to get frequencies of unigrams and bigrams"]},{"cell_type":"code","metadata":{"id":"vizo7IM-Mhxx","colab_type":"code","colab":{}},"source":["def createBigram(data):\n","\tlistOfBigrams = []\n","\tbigramCounts = {}\n","\tunigramCounts = {}\n","\n","\tfor i in range(len(data)):\n","\t\tif i < len(data) - 1:\n","\n","\t\t\tlistOfBigrams.append((data[i], data[i + 1]))\n","\n","\t\t\tif (data[i], data[i+1]) in bigramCounts:\n","\t\t\t\tbigramCounts[(data[i], data[i + 1])] += 1\n","\t\t\telse:\n","\t\t\t\tbigramCounts[(data[i], data[i + 1])] = 1\n","\n","\t\tif data[i] in unigramCounts:\n","\t\t\tunigramCounts[data[i]] += 1\n","\t\telse:\n","\t\t\tunigramCounts[data[i]] = 1\n","\n","\treturn listOfBigrams, unigramCounts, bigramCounts"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z87ncOlM7Qm-","colab_type":"text"},"source":["Method to learn bigram probabilities"]},{"cell_type":"code","metadata":{"id":"tdMfz56Al2LX","colab_type":"code","colab":{}},"source":["def calcBigramProb(listOfBigrams, unigramCounts, bigramCounts):\n","\n","\tlistOfProb = {}\n","\tfor bigram in listOfBigrams:\n","\t\tword1 = bigram[0]\n","\t\tword2 = bigram[1]\n","\t\t\n","\t\tlistOfProb[bigram] = (bigramCounts.get(bigram))/(unigramCounts.get(word1))\n","\n","\tfile = open('bigramProb.txt', 'w')\n","\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n","\n","\tfor bigrams in listOfBigrams:\n","\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams]) + ' : ' + str(listOfProb[bigrams]) + '\\n')\n","\tfile.close()\n","\n","\treturn listOfProb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"25qCUF0N7YHd","colab_type":"text"},"source":["Method to learn bigram probabilities with Add-1 Smoothing"]},{"cell_type":"code","metadata":{"id":"k9pe7LMO5exi","colab_type":"code","colab":{}},"source":["def bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts):\n","\n","\tlistOfProb = {}\n","\tcStar = {}\n","\n","\n","\tfor bigram in listOfBigrams:\n","\t\tword1 = bigram[0]\n","\t\tword2 = bigram[1]\n","\t\tlistOfProb[bigram] = (bigramCounts.get(bigram) + 1)/(unigramCounts.get(word1) + len(unigramCounts))\n","\t\tcStar[bigram] = (bigramCounts[bigram] + 1) * unigramCounts[word1] / (unigramCounts[word1] + len(unigramCounts))\n","\n","\tfile = open('addOneSmoothing.txt', 'w')\n","\tfile.write('Bigram' + '\\t\\t\\t' + 'Count' + '\\t' + 'Probability' + '\\n')\n","\n","\tfor bigrams in listOfBigrams:\n","\t\tfile.write(str(bigrams) + ' : ' + str(bigramCounts[bigrams])\n","\t\t\t\t   + ' : ' + str(listOfProb[bigrams]) + '\\n')\n","\n","\tfile.close()\n","\n","\treturn listOfProb, cStar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NUqKF63arb6b","colab_type":"code","colab":{}},"source":["# Main Program\n","\n","# Create a list of bigrams and get frequencies of unigrams and bigrams\n","listOfBigrams, unigramCounts, bigramCounts = createBigram(data_words_train)\n","\n","# Calculate bigram probabilities\n","bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n","\n","# Apply Add-1 Smoothing and calculate probabilities and get reconstructed count of bigrams\n","bigramAddOne, addOneCstar = bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ll7utCF6QRX","colab_type":"text"},"source":["### Count Sentence Probability"]},{"cell_type":"markdown","metadata":{"id":"iY5gGr1D-78E","colab_type":"text"},"source":["Input a sentence and generate bigrams"]},{"cell_type":"code","metadata":{"id":"MGd4pxsOhiV7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b5de76f9-06bc-4000-9c4e-5d001fa67a12","executionInfo":{"status":"ok","timestamp":1583670512578,"user_tz":-330,"elapsed":1662,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["input = 'we must be very careful'\n","\n","inputList = [] # list to store bigrams\n","\n","for i in range(len(input.split())-1):\n","  inputList.append((input.split()[i], input.split()[i+1]))\n","inputList"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"3uIyvjaW_c2q","colab_type":"text"},"source":["Get Bigram counts and probabilities use them to calculate probability of the input sentence"]},{"cell_type":"code","metadata":{"id":"GD5UlEA29PP0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"91189969-4f45-4299-e750-8c885200eb09","executionInfo":{"status":"ok","timestamp":1583670563753,"user_tz":-330,"elapsed":1586,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# Open a file to write output\n","output1 = open('bigramProb-OUTPUT.txt', 'w')\n","\n","# initial probability of a sentence\n","outputProb1 = 1\n","\n","output1.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n","\n","for i in range(len(inputList)):\n","\n","  # if bigram is present in the model, get updated probability\n","  if inputList[i] in bigramProb:\n","    # write bigram, its count and probability to the file\n","    output1.write(str(inputList[i]) + '\\t\\t' + str(bigramCounts[inputList[i]]) + '\\t\\t' + str(bigramProb[inputList[i]]) + '\\n')\n","    # multiply with probability of a current bigram\n","    outputProb1 *= bigramProb[inputList[i]]\n","\n","  # if bigram is not present in the model, sentence probability is zero\n","  else:\n","    output1.write(str(inputList[i]) + '\\t\\t\\t' + str(0) + '\\t\\t\\t' + str(0) + '\\n')\n","    outputProb1 *= 0\n","\n","output1.write('\\n' + 'Probablility = ' + str(outputProb1))\n","outputProb1"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.8076159793430567e-07"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"btRgkXbDEOov","colab_type":"text"},"source":["Use Add-1 Smoothed model to get the probability"]},{"cell_type":"code","metadata":{"id":"cmrxjZ899-tz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"a6c446b4-0d43-46b6-fdcc-4acef7739150","executionInfo":{"status":"ok","timestamp":1583670578498,"user_tz":-330,"elapsed":1810,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# Open a file to write output\n","output2 = open('addOneSmoothing-OUTPUT.txt', 'w')\n","\n","# initial probability of a sentence\n","outputProb2 = 1\n","\n","output2.write('Bigram\\t\\t\\t\\t' + 'Count\\t\\t\\t\\t' + 'Probability\\n\\n')\n","\n","for i in range(len(inputList)):\n","\n","  # if bigram is present in the model, get updated probability\n","  if inputList[i] in bigramAddOne:\n","    # Update probability of the sentence\n","    outputProb2 *= bigramAddOne[inputList[i]]\n","\n","    output2.write(str(inputList[i]) + '\\t\\t' + str(addOneCstar[inputList[i]]) + '\\t\\t' + str(bigramAddOne[inputList[i]]) + '\\n')\n","\n","  # if bigram is not present in the model, use unigram counts to get estimated probability\n","  else:\n","    # if first word in a bigram is not present in unigrams, add with with count 1\n","    if inputList[i][0] not in unigramCounts:\n","      unigramCounts[inputList[i][0]] = 1\n","    \n","    # calculate probability of that word\n","    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n","\n","    # # reconstructed count for the bigram\n","    addOneCStar = 1 * unigramCounts[inputList[i][0]] / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n","    \n","    # Update probability of the sentence\n","    outputProb2 *= prob\n","\n","    output2.write(str(inputList[i]) + '\\t' + str(addOneCStar) + '\\t' + str(prob) + '\\n')\n","\n","output2.write('\\n' + 'Probablility = ' + str(outputProb2))\n","outputProb2"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.2254340301928457e-14"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"bSSQl6WzEZw9","colab_type":"text"},"source":["Show the results:"]},{"cell_type":"code","metadata":{"id":"LKLRvTgC-DeC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"outputId":"bf976d8e-ec92-4f2c-f89b-e214d9d0be5e","executionInfo":{"status":"ok","timestamp":1583670586281,"user_tz":-330,"elapsed":1334,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# input sentence\n","print(input)\n","\n","# list of bigrams\n","print(inputList)\n","\n","# probability given by simple bigram model\n","print ('Bigram Model: ', outputProb1)\n","\n","# probability given by bigram model with add-1 smoothing\n","print ('Add One: ', outputProb2)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["we must be very careful\n","[('we', 'must'), ('must', 'be'), ('be', 'very'), ('very', 'careful')]\n","Bigram Model:  2.8076159793430567e-07\n","Add One:  4.2254340301928457e-14\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WnoVrDtQKSBX","colab_type":"text"},"source":["### Next Word Recommandation"]},{"cell_type":"code","metadata":{"id":"ulm4Oo55ZH3-","colab_type":"code","colab":{}},"source":["def sentence_prob_with_next_word(next_word):\n","  outputProb = 1\n","  new_bigram = (input.split()[-1], next_word)\n","  if new_bigram in bigramAddOne:\n","    outputProb *= bigramAddOne[new_bigram]\n","  else:\n","    if new_bigram[0] not in unigramCounts:\n","      unigramCounts[new_bigram[0]] = 1\n","    prob = (1) / (unigramCounts[new_bigram[0]] + len(unigramCounts))\n","    outputProb *= prob\n","  return outputProb"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RI-D6IgAR7L0","colab_type":"code","colab":{}},"source":["input = 'the investors are'\n","possible_words = ['cheated', 'happy', 'smart', 'afraid']\n","\n","inputList = []\n","outputProb = 1\n","\n","for i in range(len(input.split())-1):\n","  inputList.append((input.split()[i], input.split()[i+1]))\n","\n","\n","for i in range(len(inputList)):\n","\n","  if inputList[i] in bigramAddOne:\n","    outputProb *= bigramAddOne[inputList[i]]\n","  else:\n","    if inputList[i][0] not in unigramCounts:\n","      unigramCounts[inputList[i][0]] = 1\n","    prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n","    outputProb *= prob"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQP7k6ZhKlZw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"acb3a23f-f7ea-48c4-c1d0-c7af5998abf1","executionInfo":{"status":"ok","timestamp":1583670631634,"user_tz":-330,"elapsed":1511,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["max_prob = 0\n","index_of_next_word = -1\n","for i, word in enumerate(possible_words):\n","  final_prob = outputProb * sentence_prob_with_next_word(word)\n","  if final_prob > max_prob:\n","    max_prob = final_prob\n","    index_of_next_word = i\n","\n","print('Next Word:', possible_words[index_of_next_word])\n","print('Output Sentece:', input, possible_words[index_of_next_word])"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Next Word: happy\n","Output Sentece: the investors are happy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wvpm9Wnv-auQ","colab_type":"text"},"source":["### Word Order"]},{"cell_type":"code","metadata":{"id":"zxHSAzEx-ekP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"efa2d439-e477-4839-f58c-05db2d8cb038","executionInfo":{"status":"ok","timestamp":1583670660928,"user_tz":-330,"elapsed":1486,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["input1 = 'the market is very happy these days'\n","input2 = 'market is the happy these very days'\n","\n","\n","inputList1 = []\n","inputList2 = []\n","\n","\n","outputProb1 = 1\n","outputProb2 = 1\n","\n","\n","for i in range(len(input1.split())-1):\n","  inputList1.append((input1.split()[i], input1.split()[i+1]))\n","\n","for i in range(len(input2.split())-1):\n","  inputList2.append((input2.split()[i], input2.split()[i+1]))\n","\n","\n","for i in range(len(inputList1)):\n","  if inputList1[i] in bigramAddOne:\n","    outputProb1 *= bigramAddOne[inputList1[i]]\n","  else:\n","    if inputList1[i][0] not in unigramCounts:\n","      unigramCounts[inputList1[i][0]] = 1\n","    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n","    outputProb1 *= prob1\n","\n","\n","for i in range(len(inputList2)):\n","  if inputList2[i] in bigramAddOne:\n","    outputProb2 *= bigramAddOne[inputList2[i]]\n","  else:\n","    if inputList2[i][0] not in unigramCounts:\n","      unigramCounts[inputList2[i][0]] = 1\n","    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n","    outputProb2 *= prob2\n","\n","print (input1, ':', outputProb1)\n","print (input2, ':', outputProb2)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["the market is very happy these days : 3.0787233025784113e-22\n","market is the happy these very days : 2.5840603259051406e-24\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3xRmdeXSE3eX","colab_type":"text"},"source":["### Sentence with Homonyms: piece or peace"]},{"cell_type":"markdown","metadata":{"id":"6NEjoIYOGA61","colab_type":"text"},"source":["Bigram model with add-1 smoothing is used"]},{"cell_type":"code","metadata":{"id":"Vav3BVn3F8w_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"b94b7d9c-f8a7-4bdb-bd2e-5ee06f73a6f3","executionInfo":{"status":"ok","timestamp":1583670675179,"user_tz":-330,"elapsed":1293,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["input1 = 'its a peace of information'\n","input2 = 'its a piece of information'\n","\n","\n","inputList1 = []\n","inputList2 = []\n","\n","\n","outputProb1 = 1\n","outputProb2 = 1\n","\n","\n","for i in range(len(input1.split())-1):\n","  inputList1.append((input1.split()[i], input1.split()[i+1]))\n","\n","for i in range(len(input2.split())-1):\n","  inputList2.append((input2.split()[i], input2.split()[i+1]))\n","\n","\n","for i in range(len(inputList1)):\n","  if inputList1[i] in bigramAddOne:\n","    outputProb1 *= bigramAddOne[inputList1[i]]\n","  else:\n","    if inputList1[i][0] not in unigramCounts:\n","      unigramCounts[inputList1[i][0]] = 1\n","    prob1 = (1) / (unigramCounts[inputList1[i][0]] + len(unigramCounts))\n","    outputProb1 *= prob1\n","\n","\n","for i in range(len(inputList2)):\n","  if inputList2[i] in bigramAddOne:\n","    outputProb2 *= bigramAddOne[inputList2[i]]\n","  else:\n","    if inputList2[i][0] not in unigramCounts:\n","      unigramCounts[inputList2[i][0]] = 1\n","    prob2 = (1) / (unigramCounts[inputList2[i][0]] + len(unigramCounts))\n","    outputProb2 *= prob2\n","\n","print (input1, ':', outputProb1)\n","print (input2, ':', outputProb2)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["its a peace of information : 8.014242631151717e-19\n","its a piece of information : 3.2057856418889864e-18\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2PfddoybIYFa","colab_type":"text"},"source":["### Perplexity of Model <br>\n","Perplexity of add-1 smoothed bigram model:"]},{"cell_type":"markdown","metadata":{"id":"Cmxsw-fwIg5Y","colab_type":"text"},"source":["Method to get probability of a sentence"]},{"cell_type":"code","metadata":{"id":"xBac7Q0Sg7pG","colab_type":"code","colab":{}},"source":["def calculate_bigram_sentence_probability(input):\n","\n","  inputList = []\n","  outputProb = 1\n","\n","  for i in range(len(input)-1):\n","    inputList.append((input[i], input[i+1]))\n","\n","  for i in range(len(inputList)):\n","    if inputList[i] in bigramAddOne:\n","      outputProb *= bigramAddOne[inputList[i]]\n","    else:\n","      if inputList[i][0] not in unigramCounts:\n","        unigramCounts[inputList[i][0]] = 1\n","      prob = (1) / (unigramCounts[inputList[i][0]] + len(unigramCounts))\n","      outputProb *= prob\n","\n","  return outputProb"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8fv-8-kJLHB","colab_type":"text"},"source":["Method to get total number of bigrams"]},{"cell_type":"code","metadata":{"id":"d9i2rXyhQiU-","colab_type":"code","colab":{}},"source":["def calculate_number_of_bigrams(sentences):\n","        bigram_count = 0\n","        for sentence in sentences:\n","            # remove one for number of bigrams in sentence\n","            bigram_count += len(sentence) - 1\n","        return bigram_count"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WAlU--jrJVna","colab_type":"text"},"source":["Method to calculate perplexity of a model"]},{"cell_type":"code","metadata":{"id":"DoUq8FC7HtT8","colab_type":"code","colab":{}},"source":["def calculate_bigram_perplexity(model, sentences):\n","    number_of_bigrams = calculate_number_of_bigrams(sentences)\n","    bigram_sentence_probability_log_sum = 0\n","    for sentence in sentences:\n","        p = calculate_bigram_sentence_probability(sentence)\n","        if p != 0.0:\n","          a = math.log(p)\n","        else:\n","          a = 0\n","        bigram_sentence_probability_log_sum -= a\n","    return math.pow(2, bigram_sentence_probability_log_sum / number_of_bigrams)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TmwFEZIhJu4Q","colab_type":"text"},"source":["Perplexity of the model over train and test data:"]},{"cell_type":"code","metadata":{"id":"ydUwPXUMNQ6y","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"4f2a6a8e-b0fd-49bd-fe5b-6315545be241","executionInfo":{"status":"ok","timestamp":1583670758927,"user_tz":-330,"elapsed":15192,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["print(\"PERPLEXITY over Training Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents))\n","print(\"PERPLEXITY over Test Data:\", calculate_bigram_perplexity(bigramAddOne, data_sents_test))"],"execution_count":32,"outputs":[{"output_type":"stream","text":["PERPLEXITY over Training Data: 137.07939217258775\n","PERPLEXITY over Test Data: 168.93416630740222\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"-7iZZDMLwIGN","colab_type":"text"},"source":["### Text Generation"]},{"cell_type":"markdown","metadata":{"id":"wCsjMMDu4z9v","colab_type":"text"},"source":["Text Generation Using Simple Bigram Model"]},{"cell_type":"code","metadata":{"id":"Rm88ndODxEsU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"de874f75-48de-41df-b37e-dc8f0dc47374","executionInfo":{"status":"ok","timestamp":1583670774638,"user_tz":-330,"elapsed":1343,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# initial word\n","text = [\"there\"]\n","\n","sentence_finished = False\n"," \n","while not sentence_finished:\n","  # select a random probability threshold  \n","  r = random.random()\n","  accumulator = 0.0\n","\n","  for pair in bigramProb.keys():\n","    if pair[0] == text[-1]:\n","      accumulator += bigramProb[pair]\n","      # select words that are above the probability threshold\n","      if accumulator >= r:\n","          text.append(pair[1])\n","          break\n","\n","  if text[-1] == 'None':\n","    sentence_finished = True\n","  if len(text) > 20:\n","    sentence_finished = True\n"," \n","print (' '.join([t for t in text if t]))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["there had given time to any prominent in the transaction , 912 billion dlrs for peasants who elect Wayne specialty metal\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7yUBX2Db48aH"},"source":["Text Generation Using Simple Trigram Model"]},{"cell_type":"code","metadata":{"id":"Bhw3fGeX2cvA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"0db56be4-1ae6-4acf-8b12-7c9b3a63518b","executionInfo":{"status":"error","timestamp":1583670786175,"user_tz":-330,"elapsed":1464,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["# initial words\n","text = [\"there\", \"is\"]\n","sentence_finished = False\n"," \n","while not sentence_finished:\n","  # select a random probability threshold  \n","  r = random.random()\n","  accumulator = 0.0\n","\n","  for tpl in trigramProb.keys():\n","    if (tpl[0] == text[-2] and tpl[1] == text[-1]):\n","      accumulator += trigramProb[tpl]\n","      # select words that are above the probability threshold\n","      if accumulator >= r:\n","          text.append(tpl[2])\n","          break\n","\n","  if text[-2:] == ['None', 'None']:\n","    sentence_finished = True\n","  if len(text) > 20:\n","    sentence_finished = True\n"," \n","print (' '.join([t for t in text if t]))"],"execution_count":34,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-51f0e340f8fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mtpl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtpl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0maccumulator\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtrigramProb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtpl\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'trigramProb' is not defined"]}]},{"cell_type":"code","metadata":{"id":"3OKrbjJHoXZQ","colab_type":"code","colab":{}},"source":["def createTrigram(data):\n","\tlistOfTrigrams = []\n","\ttrigramCounts = {}\n","\tunigramCounts = {}\n","\n","\tfor i in range(len(data)):\n","\t\tif i < len(data) - 2:\n","\n","\t\t\tlistOfTrigrams.append((data[i], data[i + 1], data[i + 2]))\n","\n","\t\t\tif (data[i], data[i + 1], data[i + 2]) in trigramCounts:\n","\t\t\t\ttrigramCounts[(data[i], data[i + 1], data[i + 2])] += 1\n","\t\t\telse:\n","\t\t\t\ttrigramCounts[(data[i], data[i + 1], data[i + 2])] = 1\n","\n","\treturn listOfTrigrams, trigramCounts"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bGzJ8rEqq1k6","colab_type":"code","colab":{}},"source":["# Main Program for Trigram\n","\n","# Create a list of Trigrams and get frequencies trigrams\n","listOfTrigrams, trigramCounts = createTrigram(data_words_train)\n","\n","# # Calculate bigram probabilities\n","# bigramProb = calcBigramProb(listOfBigrams, unigramCounts, bigramCounts)\n","\n","# # Apply Add-1 Smoothing and calculate probabilities and get reconstructed count of bigrams\n","# bigramAddOne, addOneCstar = bigramWithAddOneSmoothing(listOfBigrams, unigramCounts, bigramCounts)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"htnZzmYCrIWo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"fd9df283-fd4f-448a-c410-1747c4469ad1","executionInfo":{"status":"ok","timestamp":1583670875254,"user_tz":-330,"elapsed":1638,"user":{"displayName":"MITESH GOSWAMI","photoUrl":"","userId":"08920737225917145710"}}},"source":["listOfTrigrams[:10]\n","type(trigramCounts)"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{"tags":[]},"execution_count":37}]}]}